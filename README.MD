# ğŸ­ Procurement Data Pipeline

**Module:** Fondements Big Data (ENSA Al Hoceima)  
**Academic Year:** 2025-2026  
**Branch:** `data_ing-phase3`

---

## ğŸ“Œ Project Overview

This project implements a **batch-oriented data pipeline** for a retail procurement system. The system collects daily customer orders from 15 Points of Sale (POS) and warehouse stock levels from 5 warehouses, then calculates net demand and automatically generates supplier replenishment orders.

### ğŸ¯ Business Objective

Transform raw sales and inventory data into actionable procurement decisions:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   15 Retail     â”‚     â”‚   Data Pipeline â”‚     â”‚   5 Supplier    â”‚
â”‚    Stores       â”‚ â”€â”€â–¶ â”‚   Processing    â”‚ â”€â”€â–¶ â”‚    Orders       â”‚
â”‚  (JSON Orders)  â”‚     â”‚   (Batch ETL)   â”‚     â”‚   (JSON Files)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âš ï¸ Key Constraints

| Constraint | Description |
|------------|-------------|
| **Batch Only** | No streaming technologies (Kafka/Flink/Spark Streaming) allowed |
| **Distributed** | Uses HDFS for storage and Trino for distributed SQL queries |
| **Separation** | Clearly separated OLTP (PostgreSQL) and Analytical (HDFS) layers |
| **Time Window** | Pipeline runs in 22:00 - 00:00 batch window |

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PROCUREMENT PIPELINE ARCHITECTURE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                  â”‚
â”‚   DATA SOURCES                                                                   â”‚
â”‚   â•â•â•â•â•â•â•â•â•â•â•â•                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚   â”‚   15 POS     â”‚    â”‚  5 Warehouses â”‚    â”‚  PostgreSQL  â”‚                      â”‚
â”‚   â”‚   Stores     â”‚    â”‚    Stock      â”‚    â”‚  Master Data â”‚                      â”‚
â”‚   â”‚  (JSON)      â”‚    â”‚   (CSV)       â”‚    â”‚  (Products,  â”‚                      â”‚
â”‚   â”‚              â”‚    â”‚              â”‚    â”‚  Suppliers)  â”‚                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚          â”‚                   â”‚                   â”‚                               â”‚
â”‚          â–¼                   â–¼                   â”‚                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚                               â”‚
â”‚   â”‚         HDFS DATA LAKE              â”‚       â”‚                               â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚       â”‚                               â”‚
â”‚   â”‚  â”‚ /raw/orders â”‚ â”‚ /raw/stock  â”‚   â”‚       â”‚                               â”‚
â”‚   â”‚  â”‚  105 files  â”‚ â”‚  35 files   â”‚   â”‚       â”‚                               â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚       â”‚                               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚                               â”‚
â”‚                      â”‚                           â”‚                               â”‚
â”‚                      â–¼                           â–¼                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚                      TRINO QUERY ENGINE                          â”‚           â”‚
â”‚   â”‚         (Federated SQL across HDFS + PostgreSQL)                â”‚           â”‚
â”‚   â”‚                                                                  â”‚           â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚           â”‚
â”‚   â”‚  â”‚  Hive Catalog   â”‚              â”‚ PostgreSQL      â”‚          â”‚           â”‚
â”‚   â”‚  â”‚  (HDFS Data)    â”‚              â”‚ Catalog         â”‚          â”‚           â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                              â”‚                                                   â”‚
â”‚                              â–¼                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚                    PYTHON PROCESSING                             â”‚           â”‚
â”‚   â”‚                                                                  â”‚           â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚           â”‚
â”‚   â”‚  â”‚ Aggregate  â”‚ â”‚ Calculate  â”‚ â”‚ Apply      â”‚ â”‚ Generate   â”‚   â”‚           â”‚
â”‚   â”‚  â”‚ Demand     â”‚ â”‚ Net Demand â”‚ â”‚ Business   â”‚ â”‚ Reports    â”‚   â”‚           â”‚
â”‚   â”‚  â”‚            â”‚ â”‚            â”‚ â”‚ Rules      â”‚ â”‚            â”‚   â”‚           â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                              â”‚                                                   â”‚
â”‚                              â–¼                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚                      OUTPUT LAYER                                â”‚           â”‚
â”‚   â”‚                                                                  â”‚           â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚           â”‚
â”‚   â”‚  â”‚ Supplier    â”‚  â”‚ Exception   â”‚  â”‚ Pipeline    â”‚              â”‚           â”‚
â”‚   â”‚  â”‚ Orders      â”‚  â”‚ Reports     â”‚  â”‚ Summary     â”‚              â”‚           â”‚
â”‚   â”‚  â”‚ (5 JSON)    â”‚  â”‚ (JSON+TXT)  â”‚  â”‚ (TXT)       â”‚              â”‚           â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                                  â”‚
â”‚   ORCHESTRATION                                                                  â”‚
â”‚   â•â•â•â•â•â•â•â•â•â•â•â•â•                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚                    APACHE AIRFLOW                                â”‚           â”‚
â”‚   â”‚         Schedule: Daily at 22:00 | DAG: procurement_daily       â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Airflow DAG Workflow

The pipeline orchestration follows this execution flow:

```
                            start
                            â”‚
                            â–¼
                    get_processing_date
                            â”‚
                            â–¼
                  validate_data_sources
                            â”‚
                            â–¼
                    data_quality_check
                            â”‚
                            â–¼
                     compute_demand
                            â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â–¼                     â–¼
           export_orders      generate_exceptions
                 â”‚                     â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
                    generate_summary
                            â”‚
                            â–¼
                           end
```

### Task Breakdown

| Task # | Task Name | Duration | Description |
|--------|-----------|----------|-------------|
| 1 | start_pipeline | <1s | Initialize pipeline execution |
| 2 | get_processing_date | <1s | Determine date to process (execution_date) |
| 3 | validate_data_sources | ~2s | Verify order & stock files exist |
| 4 | data_quality_check | ~5s | Validate JSON/CSV structure & data types |
| 5 | compute_demand | ~15s | Run Trino queries & calculate net demand |
| 6 | export_orders | ~8s | Generate supplier order JSON files |
| 7 | generate_exceptions | ~8s | Detect anomalies & create alerts |
| 8 | generate_summary | ~2s | Compile execution report |
| 9 | end_pipeline | <1s | Mark completion |

**Total Execution Time:** ~40-60 seconds  
**Schedule:** `0 22 * * *` (Daily at 22:00)  
**Retries:** 2 attempts with 5-minute delay  
**Timeout:** 2 hours max

---

## ğŸ’» Technical Stack

| Component | Technology | Version | Purpose |
|-----------|------------|---------|---------|
| **Container Platform** | Docker Compose | 3.x | Infrastructure orchestration |
| **Data Lake** | Apache HDFS | 3.2.1 | Distributed file storage |
| **OLTP Database** | PostgreSQL | 13 | Master data storage |
| **Query Engine** | Trino | 435 | Distributed SQL processing |
| **Orchestration** | Apache Airflow | 2.7.3 | Workflow automation |
| **Programming** | Python | 3.10+ | Data processing scripts |
| **Web UI** | pgAdmin | Latest | Database management |

### Python Dependencies

```
pandas>=2.0.0          # Data manipulation
trino>=0.327.0         # Trino Python client
psycopg2-binary>=2.9.0 # PostgreSQL client
faker>=18.0.0          # Test data generation
```

---

## ğŸ“Š Business Logic

### 1. Net Demand Formula

The core replenishment calculation:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                  â”‚
â”‚   Net Demand = Total Orders - Available Stock + Safety Stock    â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Real-World Example - Step by Step:

**Product: Whole Milk 1L (SKU-0002)**

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“… Step 1: Aggregate Orders (Last 7 Days)
   Day 1 (Dec 28): 65 units
   Day 2 (Dec 29): 72 units
   Day 3 (Dec 30): 68 units
   Day 4 (Dec 31): 80 units  â† Holiday spike
   Day 5 (Jan 01): 85 units  â† Holiday spike
   Day 6 (Jan 02): 70 units
   Day 7 (Jan 03): 80 units
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   TOTAL DEMAND: 520 units

ğŸ“¦ Step 2: Check Current Stock
   Warehouse 1: 50 units
   Warehouse 2: 60 units
   Warehouse 3: 40 units
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   TOTAL STOCK: 150 units

ğŸ“‹ Step 3: Load Business Rules (PostgreSQL)
   â€¢ Supplier: DairyFresh LLC
   â€¢ Lead Time: 3 days
   â€¢ Safety Stock: 100 units (always keep as buffer)
   â€¢ MOQ (Minimum Order): 50 units
   â€¢ Case Size: 24 units/case

ğŸ§® Step 4: Calculate Net Demand
   Basic Need = 520 - 150 = 370 units
   Add Safety Buffer = 370 + 100 = 470 units
   
ğŸ“¦ Step 5: Round to Case Size
   Cases Needed = âŒˆ470 Ã· 24âŒ‰ = 20 cases
   Final Order = 20 Ã— 24 = 480 units âœ“

âœ… RESULT: Order 480 units (20 cases) from DairyFresh LLC
   Priority: HIGH (stock < 30% of demand)
   Expected Delivery: Jan 06 (3 days)
   Cost: 480 Ã— $4.50 = $2,160.00
```

**Example Calculation:**

```
Product: Cola 2L (SKU-0012)
â”œâ”€â”€ Total Orders from 15 stores:  2,644 units
â”œâ”€â”€ Available Stock in warehouses:  774 units
â”œâ”€â”€ Safety Stock required:         100 units
â””â”€â”€ Net Demand = 2,644 - 774 + 100 = 1,970 units
```

### 2. Procurement Rules

| Rule | Description | Example |
|------|-------------|---------|
| **MOQ** | Minimum Order Quantity per supplier | Min 100 units |
| **Case Size** | Orders rounded UP to nearest case | 6 units/case |
| **Priority** | Based on order volume | >5000 = HIGH |

**Case Size Rounding Example:**

```
Net Demand: 1,970 units
Case Size: 6 units
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Cases Needed: âŒˆ1970 Ã· 6âŒ‰ = 329 cases
Order Quantity: 329 Ã— 6 = 1,974 units âœ“
```

### 3. Exception Detection

The system automatically detects anomalies:

| Exception Type | Threshold | Severity |
|----------------|-----------|----------|
| HIGH_DEMAND | > 2,000 units | ğŸ”´ HIGH |
| LOW_STOCK | Stock < 30% of demand | ğŸ”´ HIGH |
| CRITICAL_STOCK | Stock < 10% of demand | ğŸ”´ CRITICAL |
| MISSING_SUPPLIER | No supplier assigned | ğŸ”´ HIGH |
| DEMAND_STOCK_GAP | Net demand > 3x stock | ğŸŸ¡ MEDIUM |

---

## ğŸ“‚ Project Structure

```
Big-data/
â”œâ”€â”€ README.MD                          # This file
â”œâ”€â”€ TODO.MD                            # Project roadmap & task tracking
â”‚
â””â”€â”€ procurement-pipeline/
    â”‚
    â”œâ”€â”€ docker-compose.yml             # Infrastructure definition
    â”‚
    â”œâ”€â”€ airflow/
    â”‚   â”œâ”€â”€ dags/
    â”‚   â”‚   â””â”€â”€ procurement_dag.py     # Airflow DAG (22:00 schedule)
    â”‚   â””â”€â”€ logs/                      # Airflow execution logs
    â”‚
    â”œâ”€â”€ config/
    â”‚   â”œâ”€â”€ trino/
    â”‚   â”‚   â””â”€â”€ postgresql.properties  # Trino PostgreSQL catalog
    â”‚   â””â”€â”€ trino-config/
    â”‚       â””â”€â”€ config.properties      # Trino server config
    â”‚
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ raw/
    â”‚   â”‚   â”œâ”€â”€ orders/                # Daily POS order files (JSON)
    â”‚   â”‚   â”‚   â”œâ”€â”€ pos_1_2025-12-28.json
    â”‚   â”‚   â”‚   â”œâ”€â”€ pos_1_2025-12-29.json
    â”‚   â”‚   â”‚   â””â”€â”€ ... (105 files: 15 POS Ã— 7 days)
    â”‚   â”‚   â”‚
    â”‚   â”‚   â””â”€â”€ stock/                 # Daily warehouse snapshots (CSV)
    â”‚   â”‚       â”œâ”€â”€ warehouse_1_2025-12-28.csv
    â”‚   â”‚       â””â”€â”€ ... (35 files: 5 warehouses Ã— 7 days)
    â”‚   â”‚
    â”‚   â””â”€â”€ output/
    â”‚       â”œâ”€â”€ replenishment_2026-01-03.csv
    â”‚       â”œâ”€â”€ pipeline_run_2026-01-03.txt
    â”‚       â”‚
    â”‚       â”œâ”€â”€ supplier_orders/       # JSON orders per supplier
    â”‚       â”‚   â”œâ”€â”€ BevCo_Distributors_2026-01-03.json
    â”‚       â”‚   â”œâ”€â”€ DairyFresh_LLC_2026-01-03.json
    â”‚       â”‚   â”œâ”€â”€ ElectroSupply_Co_2026-01-03.json
    â”‚       â”‚   â”œâ”€â”€ FreshMart_Wholesale_2026-01-03.json
    â”‚       â”‚   â””â”€â”€ TechGear_Plus_2026-01-03.json
    â”‚       â”‚
    â”‚       â””â”€â”€ exceptions/            # Quality control reports
    â”‚           â”œâ”€â”€ exception_report_2026-01-03.json
    â”‚           â””â”€â”€ exception_summary_2026-01-03.txt
    â”‚
    â”œâ”€â”€ scripts/
    â”‚   â”œâ”€â”€ data_gen.py                # Faker-based test data generator
    â”‚   â”œâ”€â”€ generate_orders.py         # Order generation script
    â”‚   â”œâ”€â”€ ingest_hdfs.py             # HDFS data ingestion
    â”‚   â”œâ”€â”€ upload_to_hdfs.ps1         # PowerShell upload script
    â”‚   â”œâ”€â”€ validate_data_quality.py   # Data quality validation
    â”‚   â”œâ”€â”€ compute_demand.py          # Main demand calculation
    â”‚   â”œâ”€â”€ export_orders.py           # Supplier JSON generation
    â”‚   â”œâ”€â”€ generate_exceptions.py     # Anomaly detection & reporting
    â”‚   â”œâ”€â”€ run_phase4.py              # Phase 4 orchestration
    â”‚   â”œâ”€â”€ run_pipeline.py            # Master pipeline orchestrator
    â”‚   â”œâ”€â”€ test_connection.py         # Infrastructure connectivity test
    â”‚   â”œâ”€â”€ test_trino_catalog.py      # Trino catalog validation
    â”‚   â””â”€â”€ test_system.py             # Full system integration test
    â”‚
    â””â”€â”€ sql/
        â””â”€â”€ postgres/
            â”œâ”€â”€ schema.sql             # Database schema
            â””â”€â”€ init_master_data.sql   # Seed data (products, suppliers)
```

---

## ğŸš€ Quick Start Guide

### Prerequisites

- Docker Desktop installed and running
- Python 3.10+ with pip
- Git

### 1. Clone Repository

```bash
git clone https://github.com/mohamedamineelabidi/Big-data.git
cd Big-data/procurement-pipeline
```

### 2. Start Infrastructure

```bash
docker-compose up -d
```

### 3. Install Python Dependencies

```bash
pip install pandas trino psycopg2-binary faker
```

### 4. Verify Services

```bash
python scripts/test_connection.py
```

### 5. Run Full Pipeline

```bash
# Run for specific date
python scripts/run_pipeline.py --date 2026-01-03

# Or run system test
python scripts/test_system.py
```

### 6. Access Web Interfaces

| Service | URL | Credentials |
|---------|-----|-------------|
| **Airflow UI** | http://localhost:8081 | admin / admin |
| **Trino UI** | http://localhost:8080 | admin (no password) |
| **pgAdmin** | http://localhost:5050 | admin@admin.com / admin |
| **HDFS UI** | http://localhost:9870 | No auth required |

---

## ğŸ“œ Scripts Documentation

### Core Pipeline Scripts

| Script | Purpose | Usage |
|--------|---------|-------|
| `run_pipeline.py` | Master orchestrator | `python scripts/run_pipeline.py --date 2026-01-03` |
| `compute_demand.py` | Demand calculation | `python scripts/compute_demand.py` |
| `export_orders.py` | Supplier JSON export | `python scripts/export_orders.py --date 2026-01-03` |
| `generate_exceptions.py` | Exception reporting | `python scripts/generate_exceptions.py --date 2026-01-03` |

### Utility Scripts

| Script | Purpose | Usage |
|--------|---------|-------|
| `test_system.py` | Integration test (37 tests) | `python scripts/test_system.py` |
| `test_connection.py` | Service connectivity | `python scripts/test_connection.py` |
| `data_gen.py` | Generate test data | `python scripts/data_gen.py --days 7` |

### CLI Options

```bash
# Master Pipeline
python scripts/run_pipeline.py --date 2026-01-03     # Specific date
python scripts/run_pipeline.py --replay 7            # Last 7 days
python scripts/run_pipeline.py --validate-only       # Validation only
python scripts/run_pipeline.py --skip-validation     # Skip validation

# Export Orders
python scripts/export_orders.py --date 2026-01-03 --output data
```

---

## ğŸ“¦ Data Models

### Input: Order JSON Schema

```json
{
  "pos_id": "POS-001",
  "date": "2026-01-03",
  "items": [
    {
      "sku": "SKU-0012",
      "quantity": 24,
      "unit_price": 2.99
    }
  ]
}
```

### Input: Stock CSV Schema

| Column | Type | Description |
|--------|------|-------------|
| warehouse_id | string | Warehouse identifier |
| sku | string | Product SKU |
| available_stock | integer | Units available |
| reserved_stock | integer | Units reserved |
| date | date | Snapshot date |

### Output: Supplier Order JSON

```json
{
  "order_id": "ORD-20260103-006-001",
  "supplier_id": "SUP-006",
  "supplier_name": "BevCo Distributors",
  "order_date": "2026-01-03",
  "requested_delivery_date": "2026-01-05",
  "status": "PENDING",
  "priority": "HIGH",
  "items": [
    {
      "line_number": 1,
      "sku": "SKU-0001",
      "product_name": "Organic Apple Juice",
      "category": "Beverages",
      "quantity_ordered": 1344,
      "cases": 56,
      "case_size": 24,
      "net_demand": 1333.0
    }
  ],
  "summary": {
    "total_line_items": 7,
    "total_units": 10494,
    "total_cases": 874
  },
  "metadata": {
    "generated_by": "procurement_pipeline",
    "generation_timestamp": "2026-01-04T03:52:28",
    "pipeline_version": "1.0"
  }
}
```

### Output: Exception Report

```json
{
  "report_date": "2026-01-03",
  "generated_at": "2026-01-04T02:58:07",
  "summary": {
    "total_skus_analyzed": 24,
    "total_exceptions": 30,
    "by_severity": {
      "CRITICAL": 0,
      "HIGH": 28,
      "MEDIUM": 2
    }
  },
  "exceptions": [
    {
      "type": "HIGH_DEMAND",
      "severity": "HIGH",
      "sku": "SKU-0022",
      "product_name": "Bagels 6pk",
      "metric_value": 2852,
      "threshold": 2000,
      "recommendation": "Consider expedited supplier contact"
    }
  ]
}
```

---

## ğŸ“ˆ Pipeline Results

### Latest Execution (2026-01-03)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    PIPELINE EXECUTION SUMMARY                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸ“… Processing Date:     2026-01-03                                 â•‘
â•‘  â±ï¸  Execution Time:      1.06 seconds                              â•‘
â•‘  ğŸ“Š Status:              âœ… SUCCESS                                 â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸ“¦ INPUT DATA                                                      â•‘
â•‘     â€¢ Order Files:        105 JSON files                            â•‘
â•‘     â€¢ Stock Files:        35 CSV files                              â•‘
â•‘     â€¢ Order Items:        15,150 items                              â•‘
â•‘     â€¢ Stock Records:      250 records                               â•‘
â•‘     â€¢ POS Locations:      15 stores                                 â•‘
â•‘     â€¢ Warehouses:         5 locations                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸ“Š PROCESSING                                                      â•‘
â•‘     â€¢ Total Demand:       121,840 units                             â•‘
â•‘     â€¢ Total Stock:        58,401 units                              â•‘
â•‘     â€¢ Unique SKUs:        50 products                               â•‘
â•‘     â€¢ Products (Master):  49 in database                            â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸ“¤ OUTPUT DATA                                                     â•‘
â•‘     â€¢ SKUs to Reorder:    24 products                               â•‘
â•‘     â€¢ Total Units:        32,748 units                              â•‘
â•‘     â€¢ Suppliers:          5 orders generated                        â•‘
â•‘     â€¢ Exceptions:         30 (0 critical)                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Supplier Orders Generated

| Supplier | SKUs | Units | Cases | Priority |
|----------|------|-------|-------|----------|
| BevCo Distributors | 7 | 10,494 | 874 | ğŸ”´ HIGH |
| FreshMart Wholesale | 5 | 6,744 | 562 | ğŸ”´ HIGH |
| DairyFresh LLC | 5 | 6,594 | 549 | ğŸ”´ HIGH |
| ElectroSupply Co | 4 | 5,461 | 455 | ğŸ”´ HIGH |
| TechGear Plus | 3 | 3,455 | 288 | ğŸŸ¡ MEDIUM |
| **TOTAL** | **24** | **32,748** | **2,728** | |

### Top 10 Items to Reorder

| Rank | SKU | Product | Category | Order Qty | Supplier |
|------|-----|---------|----------|-----------|----------|
| 1 | SKU-0031 | USB Cable 2m | Electronics | 2,200 | ElectroSupply Co |
| 2 | SKU-0021 | Croissant Pack | Bakery | 2,040 | FreshMart Wholesale |
| 3 | SKU-0012 | Cola 2L | Beverages | 1,974 | BevCo Distributors |
| 4 | SKU-0005 | Almond Milk | Beverages | 1,728 | BevCo Distributors |
| 5 | SKU-0045 | Cream Cheese | Dairy | 1,692 | DairyFresh LLC |
| 6 | SKU-0024 | Baguette | Bakery | 1,540 | FreshMart Wholesale |
| 7 | SKU-0043 | Cheddar Cheese | Dairy | 1,510 | DairyFresh LLC |
| 8 | SKU-0003 | Wireless Mouse | Electronics | 1,440 | ElectroSupply Co |
| 9 | SKU-0022 | Bagels 6pk | Bakery | 1,424 | FreshMart Wholesale |
| 10 | SKU-0011 | Orange Juice 1L | Beverages | 1,416 | BevCo Distributors |

---

## âœ… Project Phases Completed

### Phase 1: Infrastructure Setup âœ…

- [x] Docker Compose with PostgreSQL, HDFS, Trino, pgAdmin, Airflow
- [x] PostgreSQL schema with products, suppliers, replenishment_rules
- [x] Master data: 49 products, 10 suppliers, 49 rules
- [x] All services verified and running

### Phase 2: Data Ingestion âœ…

- [x] HDFS directory hierarchy (`/raw/orders/`, `/raw/stock/`)
- [x] 7 days of test data generated (Dec 28 - Jan 3)
- [x] 105 order files (15 POS Ã— 7 days)
- [x] 35 stock files (5 warehouses Ã— 7 days)
- [x] Data quality validation implemented

### Phase 3: Analytical Processing âœ…

- [x] Trino connectors: PostgreSQL catalog, Hive catalog
- [x] Demand aggregation: 121,840 units across 50 SKUs
- [x] Net demand computation with safety stock
- [x] Business rules: MOQ and case size rounding
- [x] Result: 24 SKUs require 32,748 units

### Phase 4: Output Generation âœ…

- [x] Supplier order export (5 JSON files)
- [x] Exception reporting (30 exceptions detected)
- [x] Order priorities assigned (4 HIGH, 1 MEDIUM)
- [x] Human-readable reports generated

### Phase 5: Orchestration âœ…

- [x] Master orchestrator (`run_pipeline.py`)
- [x] Airflow DAG scheduled at 22:00 daily
- [x] Historical replay capability (`--replay N`)
- [x] System integration tests (37 tests, 100% pass)

---

## ğŸ§ª System Tests

Run the comprehensive test suite:

```bash
python scripts/test_system.py
```

### Test Categories

| Category | Tests | Description |
|----------|-------|-------------|
| Docker Services | 5 | Container health checks |
| PostgreSQL | 4 | Connection & data validation |
| Trino | 2 | Query engine verification |
| Data Files | 3 | File existence & counts |
| Demand Module | 4 | Data loading & processing |
| Export Module | 4 | JSON generation |
| Exception Module | 4 | Report generation |
| Orchestrator | 3 | Infrastructure validation |
| Airflow DAG | 4 | DAG syntax & configuration |
| End-to-End | 4 | Output file verification |
| **TOTAL** | **37** | **All passing âœ…** |

---

## ğŸ³ Docker Services

```yaml
services:
  postgres:        # Master data storage
  namenode:        # HDFS name node
  datanode:        # HDFS data node
  trino:           # Query engine
  pgadmin:         # Database UI
  airflow:         # Workflow orchestration
```

### Service Ports

| Service | Internal Port | External Port |
|---------|---------------|---------------|
| PostgreSQL | 5432 | 5432 |
| HDFS Namenode | 9000, 9870 | 9000, 9870 |
| Trino | 8080 | 8080 |
| Airflow | 8080 | 8081 |
| pgAdmin | 80 | 5050 |

---

## ğŸ”§ Troubleshooting

### Common Issues

**1. Airflow not starting**
```bash
# Create airflow database
docker exec procurement_postgres psql -U admin -d procurement_db -c "CREATE DATABASE airflow_db;"
docker restart procurement_airflow
```

**2. Trino connection refused**
```bash
# Check Trino logs
docker logs procurement_trino
# Verify config
cat config/trino-config/config.properties
```

**3. HDFS not accessible**
```bash
# Check HDFS health
docker logs procurement_namenode
# Verify safe mode
docker exec procurement_namenode hdfs dfsadmin -safemode get
```

**4. PostgreSQL connection issues**
```bash
# Test connection
docker exec procurement_postgres psql -U admin -d procurement_db -c "SELECT 1"
```

---

## ğŸ¤– AI Context (GitHub Copilot)

When working with this project, Copilot should:

- **Variable Naming:** Use clear business terms (`sku`, `moq`, `safety_stock`, `net_demand`)
- **Context:** This is a Data Engineering / ETL project, not a web application
- **Constraint:** No streaming - batch processing only
- **Query Engine:** Use `trino` Python client, not `prestodb`
- **Data Format:** JSON for orders, CSV for stock, PostgreSQL for master data

---

## ğŸ‘¥ Team

**Project:** Big Data Fundamentals (Fondements Big Data)  
**Institution:** ENSA Al Hoceima  
**Academic Year:** 2025-2026

---

## ğŸ“„ License

Academic project - ENSA Al Hoceima Â© 2025-2026
