<div align="center">

# Big Data Procurement Pipeline

### Intelligent Retail Replenishment System

![Status](https://img.shields.io/badge/status-production-brightgreen)
![Docker](https://img.shields.io/badge/docker-20.10+-blue)
![Python](https://img.shields.io/badge/python-3.10+-blue)
![Airflow](https://img.shields.io/badge/airflow-2.7.3-orange)
![HDFS](https://img.shields.io/badge/hadoop-3.2.1-yellow)
![Trino](https://img.shields.io/badge/trino-435+-purple)

---

</div>

## Table of Contents

- [Project Overview](#project-overview)
- [System Architecture](#system-architecture)
- [Technology Stack](#technology-stack)
- [Key Features](#key-features)
- [Quick Start](#quick-start)
- [Pipeline Workflow](#pipeline-workflow)
- [Business Logic](#business-logic)
- [Data Flow](#data-flow)
- [Project Statistics](#project-statistics)
- [Usage](#usage)
- [Monitoring](#monitoring)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)
- [Contact](#contact)

---

## Project Overview

This project implements a **production-grade, batch-oriented data pipeline** for automated retail procurement. The system processes daily customer orders from 15 Points of Sale (POS) and warehouse stock levels from 5 warehouses, calculating net demand and automatically generating optimized supplier replenishment orders.

### Business Objective

Transform raw transactional data into intelligent procurement decisions using distributed computing:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   15 Retail     ‚îÇ     ‚îÇ   Data Pipeline ‚îÇ     ‚îÇ   5 Supplier    ‚îÇ
‚îÇ    Stores       ‚îÇ ‚îÄ‚îÄ‚ñ∂ ‚îÇ   Processing    ‚îÇ ‚îÄ‚îÄ‚ñ∂ ‚îÇ    Orders       ‚îÇ
‚îÇ  (JSON Orders)  ‚îÇ     ‚îÇ   (Batch ETL)   ‚îÇ     ‚îÇ   (JSON Files)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      Daily                 22:00-00:00              Automated
```

### Key Constraints

| Constraint | Description |
|------------|-------------|
| **Batch Only** | No streaming technologies (Kafka/Flink/Spark Streaming) |
| **Distributed** | HDFS 3-way replication across 4 datanodes |
| **Separation** | Clear OLTP (PostgreSQL) and Analytical (HDFS) layers |
| **Time Window** | Daily batch processing: 22:00 - 00:00 |
| **Scalable** | Handles 141+ files, 49 products, 10 suppliers |

---

## System Architecture

### Architecture Diagram

![System Architecture](procurement-pipeline/images/architectureproject.png)

*Figure 1: Complete data flow from sources through processing to outputs*

### Infrastructure Overview

The system runs on **8 Docker containers** forming a complete big data ecosystem:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      PROCUREMENT PIPELINE ARCHITECTURE                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                                  ‚îÇ
‚îÇ   DATA SOURCES                                                                   ‚îÇ
‚îÇ   ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                                                                   ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ
‚îÇ   ‚îÇ   15 POS     ‚îÇ    ‚îÇ  5 Warehouses ‚îÇ    ‚îÇ  PostgreSQL  ‚îÇ                      ‚îÇ
‚îÇ   ‚îÇ   Stores     ‚îÇ    ‚îÇ    Stock      ‚îÇ    ‚îÇ  Master Data ‚îÇ                      ‚îÇ
‚îÇ   ‚îÇ  (105 JSON)  ‚îÇ    ‚îÇ   (35 CSV)    ‚îÇ    ‚îÇ  (Products,  ‚îÇ                      ‚îÇ
‚îÇ   ‚îÇ              ‚îÇ    ‚îÇ              ‚îÇ    ‚îÇ  Suppliers)  ‚îÇ                      ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ
‚îÇ          ‚îÇ                   ‚îÇ                   ‚îÇ                               ‚îÇ
‚îÇ          ‚ñº                   ‚ñº                   ‚îÇ                               ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ                               ‚îÇ
‚îÇ   ‚îÇ     HDFS DATA LAKE (3.93 TB)        ‚îÇ       ‚îÇ                               ‚îÇ
‚îÇ   ‚îÇ   üîÑ 3-Way Replication (4 Nodes)    ‚îÇ       ‚îÇ                               ‚îÇ
‚îÇ   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ       ‚îÇ                               ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ /raw/orders ‚îÇ ‚îÇ /raw/stock  ‚îÇ   ‚îÇ       ‚îÇ                               ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ  105 files  ‚îÇ ‚îÇ  35 files   ‚îÇ   ‚îÇ       ‚îÇ                               ‚îÇ
‚îÇ   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ       ‚îÇ                               ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ                               ‚îÇ
‚îÇ                      ‚îÇ                           ‚îÇ                               ‚îÇ
‚îÇ                      ‚ñº                           ‚ñº                               ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ   ‚îÇ                      TRINO QUERY ENGINE                          ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ            Federated SQL across HDFS + PostgreSQL                ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ                                                                  ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ  Hive Catalog   ‚îÇ              ‚îÇ PostgreSQL      ‚îÇ          ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ  (HDFS Data)    ‚îÇ              ‚îÇ Catalog         ‚îÇ          ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ           ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ                              ‚îÇ                                                   ‚îÇ
‚îÇ                              ‚ñº                                                   ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ   ‚îÇ                    PYTHON PROCESSING                             ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ                                                                  ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ Aggregate  ‚îÇ ‚îÇ Calculate  ‚îÇ ‚îÇ Apply      ‚îÇ ‚îÇ Generate   ‚îÇ   ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ Demand     ‚îÇ ‚îÇ Net Demand ‚îÇ ‚îÇ Business   ‚îÇ ‚îÇ Reports    ‚îÇ   ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ            ‚îÇ ‚îÇ            ‚îÇ ‚îÇ Rules      ‚îÇ ‚îÇ            ‚îÇ   ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ           ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ                              ‚îÇ                                                   ‚îÇ
‚îÇ                              ‚ñº                                                   ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ   ‚îÇ                      OUTPUT LAYER                                ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ                                                                  ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ Supplier    ‚îÇ  ‚îÇ Exception   ‚îÇ  ‚îÇ Pipeline    ‚îÇ              ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ Orders      ‚îÇ  ‚îÇ Reports     ‚îÇ  ‚îÇ Summary     ‚îÇ              ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ (5 JSON)    ‚îÇ  ‚îÇ (JSON+TXT)  ‚îÇ  ‚îÇ (TXT)       ‚îÇ              ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ           ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ                                                                                  ‚îÇ
‚îÇ   ORCHESTRATION                                                                  ‚îÇ
‚îÇ   ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                                                                  ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ   ‚îÇ                    APACHE AIRFLOW                                ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ       üïê Schedule: Daily at 22:00 | 9-Task DAG Pipeline          ‚îÇ           ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ                                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Container Topology

![Docker Containers](procurement-pipeline/images/docker_containers.png)

*Figure 2: Running Docker containers with health status*

| Container | Image | Purpose | Ports | Status |
|-----------|-------|---------|-------|--------|
| **procurement_postgres** | postgres:13 | OLTP master data | 5432 | Active |
| **procurement_namenode** | hadoop-namenode:3.2.1 | HDFS coordinator | 9870, 9000 | Active |
| **procurement_datanode** | hadoop-datanode:3.2.1 | HDFS legacy storage node | 9864 | Active |
| **procurement_datanode1** | hadoop-datanode:3.2.1 | HDFS storage node 1 | 9864 | Active |
| **procurement_datanode2** | hadoop-datanode:3.2.1 | HDFS storage node 2 | 9865 | Active |
| **procurement_datanode3** | hadoop-datanode:3.2.1 | HDFS storage node 3 | 9866 | Active |
| **procurement_trino** | trinodb/trino:479 | Federated query engine | 8080 | Active |
| **procurement_airflow** | apache/airflow:2.7.3 | Workflow orchestration | 8081 | Active |
| **procurement_pgadmin** | pgadmin4:latest | Database UI | 5050 | Active |

### HDFS Data Lake

![HDFS Filesystem](procurement-pipeline/images/hdfs_datanodes.png)

*Figure 3: HDFS directory structure with replicated data*

**Configuration:**
- **Replication Factor**: 3 (high availability)
- **Total Capacity**: 3.93 TB across 4 datanodes
- **Data Structure**:
  - `/raw/orders/` - Daily POS transaction files (JSON)
  - `/raw/stock/` - Daily warehouse inventory (CSV)
  - `/processed/` - Transformed data
  - `/output/` - Generated reports

### Database Schema

![Database Schema](procurement-pipeline/images/pgadmin.png)

*Figure 4: PostgreSQL master data tables*

---

## Technology Stack

### Core Technologies

| Component | Technology | Version | Purpose |
|-----------|------------|---------|---------|
| **Container Platform** | Docker Compose | 3.x | Infrastructure orchestration |
| **Data Lake** | Apache HDFS | 3.2.1 | Distributed file storage with 3-way replication |
| **OLTP Database** | PostgreSQL | 13 | Master data storage (products, suppliers) |
| **Query Engine** | Trino | 479 | Federated SQL across HDFS + PostgreSQL |
| **Orchestration** | Apache Airflow | 2.7.3 | Workflow automation & scheduling |
| **Programming** | Python | 3.10+ | Data processing & business logic |
| **Web UI** | pgAdmin | Latest | Database management interface |

### Python Dependencies

```txt
# Core Data Processing
pandas>=2.0.0              # Data manipulation & analysis
trino>=0.327.0             # Trino Python client
psycopg2-binary>=2.9.0     # PostgreSQL adapter
python-dateutil>=2.8.0     # Date/time utilities

# Data Generation & Testing
faker>=18.0.0              # Synthetic data generation

# HDFS Integration
hdfs>=2.6.0                # Python HDFS client
```

### Why These Technologies?

**HDFS (Hadoop Distributed File System)**
- Fault Tolerance: 3-way replication across 4 datanodes (3.93 TB total)
- Scalability: Handles 141+ files seamlessly, can scale to petabytes
- Industry Standard: Battle-tested for big data storage
- Academic Requirement: Fulfills distributed storage constraint

**Trino (formerly PrestoSQL)**
- Federated Queries: Single SQL interface for HDFS + PostgreSQL
- No ETL Required: Query data in place without movement
- High Performance: Optimized for analytical workloads
- ‚úÖ **Standard SQL**: ANSI SQL compliance for easy adoption

#### üóÑÔ∏è PostgreSQL
- ‚úÖ **ACID Compliance**: Guaranteed data consistency for master data
- ‚úÖ **Rich Features**: Advanced data types, constraints, triggers
- ‚úÖ **Reliability**: 20+ years of production stability
- ‚úÖ **OLTP Optimized**: Perfect for transactional master data

#### üîÑ Apache Airflow
- ‚úÖ **Visual DAGs**: Easy-to-understand workflow representation
- ‚úÖ **Robust Scheduling**: Cron-based scheduling with retry logic
- ‚úÖ **Monitoring**: Built-in logging and alerting
- ‚úÖ **Python Native**: Write workflows in Python for maximum flexibility

---

## ‚ö° Key Features
## Key Features

- **Fully Automated**: Nightly batch processing with zero manual intervention
- **High Availability**: 3-way HDFS replication across 4 datanodes (3.93 TB)
- **Fault Tolerant**: Automatic retry logic and error recovery
- **Scalable**: Handles 141+ files, 49 products, 10 suppliers effortlessly
- **Intelligent**: Business rules for MOQ, case sizes, lead times, priorities
- **Monitored**: Real-time Airflow UI with task status and logs
- **Federated Queries**: Trino queries across HDFS + PostgreSQL without ETL
- **Exception Detection**: Automatic anomaly alerts for procurement team
- **Production Ready**: Docker-based with health checks and logging
- **Easy Setup**: One-command deployment with `setup.ps1` or `setup.sh`

---

## Quick Start

### Prerequisites

```bash
# Required software
- Docker Desktop 20.10+ (with Docker Compose)
- Python 3.10+
- Git
- 8GB+ RAM recommended
- 10GB+ disk space
```

### Installation

#### Windows (PowerShell)

```powershell
# Clone the repository
git clone https://github.com/your-repo/big-data-procurement.git
cd big-data-procurement/procurement-pipeline

# Run automated setup script
.\setup.ps1

# This script will:
# ‚úì Create Python virtual environment
# ‚úì Install Python dependencies
# ‚úì Generate .gitignore
# ‚úì Create data directories
# ‚úì Generate sample data
# ‚úì Start Docker containers
# ‚úì Initialize HDFS
# ‚úì Load master data to PostgreSQL
# ‚úì Upload files to HDFS
```

#### Linux/macOS (Bash)

```bash
# Clone the repository
git clone https://github.com/your-repo/big-data-procurement.git
cd big-data-procurement/procurement-pipeline

# Make script executable and run
chmod +x setup.sh
./setup.sh

# This script will:
# ‚úì Create Python virtual environment
# ‚úì Install Python dependencies
# ‚úì Generate .gitignore
# ‚úì Create data directories
# ‚úì Generate sample data
# ‚úì Start Docker containers
# ‚úì Initialize HDFS
# ‚úì Load master data to PostgreSQL
# ‚úì Upload files to HDFS
```

### Verify Installation

```bash
# Check all containers are running
docker ps

# Expected output: 8 containers running
# procurement_postgres
# procurement_namenode
# procurement_datanode
# procurement_datanode1
# procurement_datanode2
# procurement_datanode3
# procurement_trino
# procurement_airflow
# procurement_pgadmin

# Check HDFS health
docker exec procurement_namenode hdfs dfsadmin -report

# Access web interfaces
# Airflow UI:  http://localhost:8081 (admin/admin)
# Trino UI:    http://localhost:8080
# HDFS UI:     http://localhost:9870
# pgAdmin:     http://localhost:5050 (admin@admin.com/admin)
```

---

## Project Statistics

### Data Volume

| Metric | Count | Details |
|--------|-------|---------|
| **Total Files** | 141 | 105 JSON + 35 CSV + 1 test file |
| **Order Files** | 105 | 15 stores √ó 7 days of history |
| **Stock Files** | 35 | 5 warehouses √ó 7 days of history |
| **Products** | 49 | Across 5 categories (Dairy, Bakery, Beverages, Snacks, Frozen) |
| **Suppliers** | 10 | With unique lead times and MOQs |
| **POS Stores** | 15 | Retail locations generating daily orders |
| **Warehouses** | 5 | Distribution centers tracking stock |
| **HDFS Capacity** | 3.93 TB | Across 4 datanodes with 3-way replication |
| **Pipeline Tasks** | 9 | Airflow DAG tasks |
| **Schedule** | 22:00 Daily | Batch processing window |

### Sample Products

| SKU | Product | Category | Supplier | Lead Time |
|-----|---------|----------|----------|-----------|
| SKU-0001 | Skim Milk 1L | Dairy | DairyFresh LLC | 3 days |
| SKU-0015 | Apple Juice 1L | Beverages | BevCo Distributors | 2 days |
| SKU-0022 | Bagels 6pk | Bakery | FreshMart Wholesale | 1 day |
| SKU-0030 | Ice Cream Tub 1L | Frozen | FreshMart Wholesale | 5 days |
| SKU-0042 | Potato Chips 200g | Snacks | FreshMart Wholesale | 2 days |

### HDFS Replication Details

```
Cluster Configuration:
‚Ä¢ Replication Factor: 3
‚Ä¢ Total Datanodes: 4 (1 legacy + 3 new)
‚Ä¢ Active Nodes: 4
‚Ä¢ Total Capacity: 3.93 TB
‚Ä¢ Configured Capacity: 3,869.74 GB
‚Ä¢ Status: HEALTHY

Datanode Distribution:
‚Ä¢ datanode  (legacy): 1006.85 GB | 140 blocks
‚Ä¢ datanode1 (new):    1006.85 GB | Active
‚Ä¢ datanode2 (new):    1006.85 GB | Active
‚Ä¢ datanode3 (new):    1006.85 GB | Active

Replication Health:
‚Ä¢ Average Replication: 3.0
‚Ä¢ Under-replicated: 0
‚Ä¢ Missing Blocks: 0
‚Ä¢ Corrupt Blocks: 0
```

---

## Pipeline Workflow

### Airflow DAG Visualization

![Airflow DAG Graph](procurement-pipeline/images/airflow_dag_graph.png)

*Figure 5: Airflow pipeline execution graph with 9 tasks*

The procurement pipeline consists of **9 orchestrated tasks** executed daily at 22:00:

```
                          start
                            ‚îÇ
                            ‚ñº
                    get_processing_date
                            ‚îÇ
                            ‚ñº
                  validate_data_sources
                            ‚îÇ
                            ‚ñº
                    data_quality_check
                            ‚îÇ
                            ‚ñº
                     compute_demand
                            ‚îÇ
                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                 ‚ñº                     ‚ñº
           export_orders      generate_exceptions
                 ‚îÇ                     ‚îÇ
                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚ñº
                    generate_summary
                            ‚îÇ
                            ‚ñº
                           end
```

### Task Breakdown

| Task # | Task Name | Duration | Description |
|--------|-----------|----------|-------------|
| 1 | `start_pipeline` | <1s | Initialize pipeline execution |
| 2 | `get_processing_date` | <1s | Determine date to process (execution_date) |
| 3 | `validate_data_sources` | ~2s | Verify order & stock files exist |
| 4 | `data_quality_check` | ~5s | Validate JSON/CSV structure & data types |
| 5 | `compute_demand` | ~15s | Run Trino queries & calculate net demand |
| 6 | `export_orders` | ~8s | Generate supplier order JSON files |
| 7 | `generate_exceptions` | ~8s | Detect anomalies & create alerts |
| 8 | `generate_summary` | ~2s | Compile execution report |
| 9 | `end_pipeline` | <1s | Mark completion |

**Total Execution Time**: ~40-60 seconds  
**Schedule**: `0 22 * * *` (Daily at 22:00)  
**Retries**: 2 attempts with 5-minute delay  
**Timeout**: 2 hours max

---

## Business Logic

### 1. Net Demand Formula

The core replenishment calculation:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                  ‚îÇ
‚îÇ   Net Demand = Total Orders - Available Stock + Safety Stock    ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Real-World Example - Step by Step:**

```
Product: Whole Milk 1L (SKU-0002)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üìÖ Step 1: Aggregate Orders (Last 7 Days)
   Day 1 (Dec 28): 65 units
   Day 2 (Dec 29): 72 units
   Day 3 (Dec 30): 68 units
   Day 4 (Dec 31): 80 units  ‚Üê Holiday spike
   Day 5 (Jan 01): 85 units  ‚Üê Holiday spike
   Day 6 (Jan 02): 70 units
   Day 7 (Jan 03): 80 units
   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   TOTAL DEMAND: 520 units

üì¶ Step 2: Check Current Stock
   Warehouse 1: 50 units
   Warehouse 2: 60 units
   Warehouse 3: 40 units
   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   TOTAL STOCK: 150 units

üìã Step 3: Load Business Rules (PostgreSQL)
   ‚Ä¢ Supplier: DairyFresh LLC
   ‚Ä¢ Lead Time: 3 days
   ‚Ä¢ Safety Stock: 100 units (always keep as buffer)
   ‚Ä¢ MOQ (Minimum Order): 50 units
   ‚Ä¢ Case Size: 24 units/case

üßÆ Step 4: Calculate Net Demand
   Basic Need = 520 - 150 = 370 units
   Add Safety Buffer = 370 + 100 = 470 units
   
üì¶ Step 5: Round to Case Size
   Cases Needed = ‚åà470 √∑ 24‚åâ = 20 cases
   Final Order = 20 √ó 24 = 480 units ‚úì

‚úÖ RESULT: Order 480 units (20 cases) from DairyFresh LLC
   Priority: HIGH (stock < 30% of demand)
   Expected Delivery: Jan 06 (3 days)
   Cost: 480 √ó $4.50 = $2,160.00
```

### 2. Procurement Rules

| Rule | Description | Example |
|------|-------------|---------|
| **MOQ** | Minimum Order Quantity per supplier | Min 50 units |
| **Case Size** | Orders rounded UP to nearest case | 24 units/case ‚Üí ‚åà470√∑24‚åâ = 20 cases |
| **Priority** | Based on stock coverage | Stock < 30% of demand = HIGH |
| **Lead Time** | Days until delivery | 3 days for dairy products |

**Priority Assignment Logic:**

```python
if current_stock < (demand * 0.10):
    priority = "CRITICAL"     # Less than 10% - Urgent!
elif current_stock < (demand * 0.30):
    priority = "HIGH"         # Less than 30% - Important
elif order_quantity > 5000:
    priority = "HIGH"         # Large order - Plan ahead
else:
    priority = "MEDIUM"       # Standard order
```

### 3. Exception Detection

The system automatically detects anomalies and alerts procurement managers:

| Exception Type | Threshold | Severity | Action Required |
|----------------|-----------|----------|-----------------|
| **HIGH_DEMAND** | > 2,000 units | üî¥ HIGH | Verify with sales team, check for promotions |
| **LOW_STOCK** | Stock < 30% of demand | üî¥ HIGH | Expedite order, consider air freight |
| **CRITICAL_STOCK** | Stock < 10% of demand | üî¥ CRITICAL | Emergency order, notify management |
| **MISSING_SUPPLIER** | No supplier assigned | üî¥ HIGH | Contact procurement to assign supplier |
| **HIGH_VALUE_ORDER** | Order value > $10,000 | üü° MEDIUM | Manager approval required |
| **DEMAND_STOCK_GAP** | Demand > 5x stock | üü° MEDIUM | Review forecasting model |

**Real Exception Example:**

```json
{
  "exception_id": "EXC-20260103-015",
  "timestamp": "2026-01-03T22:00:45",
  "severity": "HIGH",
  "type": "HIGH_DEMAND",
  "sku": "SKU-0022",
  "product_name": "Bagels 6pk",
  "category": "Bakery",
  "metric_value": 2852,
  "threshold": 2000,
  "percentage_over": 42.6,
  "recommendation": "Unusual demand spike detected. Verify with sales team before ordering. Check for promotions or special events.",
  "context": {
    "normal_weekly_demand": 1400,
    "current_weekly_demand": 2852,
    "variance": "+103.7%"
  }
}
```

---

## Pipeline Execution Flow

### Complete Data Journey

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DAY N: Data Collection (09:00 - 21:00)                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                ‚îÇ
‚îÇ  09:00  Stores open, customers start shopping                 ‚îÇ
‚îÇ         ‚Ä¢ POS systems record transactions                      ‚îÇ
‚îÇ         ‚Ä¢ Each sale creates order items                        ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  12:00  Lunch rush - high transaction volume                  ‚îÇ
‚îÇ         ‚Ä¢ Average 500 transactions/hour across 15 stores      ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  18:00  Evening peak - highest volume                         ‚îÇ
‚îÇ         ‚Ä¢ Average 800 transactions/hour                        ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  21:00  Stores close                                          ‚îÇ
‚îÇ         ‚Ä¢ Daily order files generated                          ‚îÇ
‚îÇ         ‚Ä¢ pos_1_2026-01-03.json created (per store)           ‚îÇ
‚îÇ         ‚Ä¢ Files contain all day's transactions                 ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  21:30  Warehouses complete daily count                       ‚îÇ
‚îÇ         ‚Ä¢ Physical inventory verification                      ‚îÇ
‚îÇ         ‚Ä¢ warehouse_1_stock_2026-01-03.csv created            ‚îÇ
‚îÇ         ‚Ä¢ Contains stock levels for all SKUs                   ‚îÇ
‚îÇ                                                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DAY N: Pipeline Execution (22:00 - 23:59)                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                ‚îÇ
‚îÇ  22:00:00  üöÄ Airflow Triggers Pipeline                       ‚îÇ
‚îÇ            ‚îú‚îÄ Check: All 15 order files present?              ‚îÇ
‚îÇ            ‚îú‚îÄ Check: All 5 stock files present?               ‚îÇ
‚îÇ            ‚îî‚îÄ Check: Services healthy?                         ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  22:00:05  üìä Stage 1: Data Validation                        ‚îÇ
‚îÇ            ‚îú‚îÄ Validate JSON structure (105 order files)       ‚îÇ
‚îÇ            ‚îú‚îÄ Validate CSV format (35 stock files)            ‚îÇ
‚îÇ            ‚îú‚îÄ Check for missing fields                         ‚îÇ
‚îÇ            ‚îú‚îÄ Validate data types                             ‚îÇ
‚îÇ            ‚îî‚îÄ Result: ‚úÖ All files valid                       ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  22:00:10  üîç Stage 2: Demand Calculation                     ‚îÇ
‚îÇ            ‚îÇ                                                   ‚îÇ
‚îÇ            ‚îú‚îÄ Step 2.1: Load Orders (via Trino)              ‚îÇ
‚îÇ            ‚îÇ   Query: SELECT * FROM hive.default.orders       ‚îÇ
‚îÇ            ‚îÇ   WHERE date = '2026-01-03'                      ‚îÇ
‚îÇ            ‚îÇ   Result: 15,150 order items loaded              ‚îÇ
‚îÇ            ‚îÇ                                                   ‚îÇ
‚îÇ            ‚îú‚îÄ Step 2.2: Aggregate by SKU                      ‚îÇ
‚îÇ            ‚îÇ   Query: SELECT sku, SUM(quantity)              ‚îÇ
‚îÇ            ‚îÇ   GROUP BY sku                                   ‚îÇ
‚îÇ            ‚îÇ   Result: 50 unique SKUs with totals            ‚îÇ
‚îÇ            ‚îÇ                                                   ‚îÇ
‚îÇ            ‚îú‚îÄ Step 2.3: Load Stock (via Trino)               ‚îÇ
‚îÇ            ‚îÇ   Query: SELECT * FROM hive.default.stock       ‚îÇ
‚îÇ            ‚îÇ   WHERE date = '2026-01-03'                      ‚îÇ
‚îÇ            ‚îÇ   Result: 250 stock records loaded               ‚îÇ
‚îÇ            ‚îÇ                                                   ‚îÇ
‚îÇ            ‚îú‚îÄ Step 2.4: Load Master Data (PostgreSQL)        ‚îÇ
‚îÇ            ‚îÇ   ‚Ä¢ Products table: 49 products                  ‚îÇ
‚îÇ            ‚îÇ   ‚Ä¢ Suppliers table: 10 suppliers                ‚îÇ
‚îÇ            ‚îÇ   ‚Ä¢ Rules table: 49 replenishment rules          ‚îÇ
‚îÇ            ‚îÇ                                                   ‚îÇ
‚îÇ            ‚îú‚îÄ Step 2.5: Calculate Net Demand                  ‚îÇ
‚îÇ            ‚îÇ   Formula per SKU:                               ‚îÇ
‚îÇ            ‚îÇ   Net = Total Orders - Stock + Safety Stock     ‚îÇ
‚îÇ            ‚îÇ   Example: 520 - 150 + 100 = 470 units          ‚îÇ
‚îÇ            ‚îÇ                                                   ‚îÇ
‚îÇ            ‚îú‚îÄ Step 2.6: Apply Business Rules                  ‚îÇ
‚îÇ            ‚îÇ   ‚Ä¢ Round to MOQ multiples                       ‚îÇ
‚îÇ            ‚îÇ   ‚Ä¢ Round to case sizes                          ‚îÇ
‚îÇ            ‚îÇ   ‚Ä¢ Assign priorities                            ‚îÇ
‚îÇ            ‚îÇ                                                   ‚îÇ
‚îÇ            ‚îî‚îÄ Result: 24 SKUs need 32,748 units               ‚îÇ
‚îÇ               Output: replenishment_2026-01-03.csv            ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  22:00:35  üì¶ Stage 3: Export Supplier Orders (Parallel)      ‚îÇ
‚îÇ            ‚îú‚îÄ Load replenishment CSV                          ‚îÇ
‚îÇ            ‚îú‚îÄ Group by supplier_name                          ‚îÇ
‚îÇ            ‚îú‚îÄ Generate 5 supplier JSON files:                 ‚îÇ
‚îÇ            ‚îÇ   ‚îú‚îÄ BevCo_Distributors_2026-01-03.json         ‚îÇ
‚îÇ            ‚îÇ   ‚îú‚îÄ DairyFresh_LLC_2026-01-03.json             ‚îÇ
‚îÇ            ‚îÇ   ‚îú‚îÄ ElectroSupply_Co_2026-01-03.json           ‚îÇ
‚îÇ            ‚îÇ   ‚îú‚îÄ FreshMart_Wholesale_2026-01-03.json        ‚îÇ
‚îÇ            ‚îÇ   ‚îî‚îÄ TechGear_Plus_2026-01-03.json              ‚îÇ
‚îÇ            ‚îî‚îÄ Result: ‚úÖ 5 orders generated                    ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  22:00:35  ‚ö†Ô∏è  Stage 4: Exception Detection (Parallel)        ‚îÇ
‚îÇ            ‚îú‚îÄ HIGH_DEMAND check: > 2,000 units               ‚îÇ
‚îÇ            ‚îÇ   Found: 7 SKUs                                  ‚îÇ
‚îÇ            ‚îú‚îÄ LOW_STOCK check: stock < 30% demand            ‚îÇ
‚îÇ            ‚îÇ   Found: 18 SKUs                                 ‚îÇ
‚îÇ            ‚îú‚îÄ MISSING_SUPPLIER check                          ‚îÇ
‚îÇ            ‚îÇ   Found: 0 SKUs                                  ‚îÇ
‚îÇ            ‚îú‚îÄ HIGH_VALUE_ORDER check: > $10,000              ‚îÇ
‚îÇ            ‚îÇ   Found: 3 orders                                ‚îÇ
‚îÇ            ‚îú‚îÄ DEMAND_STOCK_GAP check: demand > 5x stock     ‚îÇ
‚îÇ            ‚îÇ   Found: 2 SKUs                                  ‚îÇ
‚îÇ            ‚îî‚îÄ Result: 30 exceptions detected                  ‚îÇ
‚îÇ               Output: exception_report_2026-01-03.json        ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  22:01:00  üìù Stage 5: Generate Summary                       ‚îÇ
‚îÇ            ‚îú‚îÄ Collect all stage metrics                       ‚îÇ
‚îÇ            ‚îú‚îÄ Format execution report                         ‚îÇ
‚îÇ            ‚îú‚îÄ Calculate performance stats                     ‚îÇ
‚îÇ            ‚îî‚îÄ Output: pipeline_run_2026-01-03.txt             ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  22:01:06  ‚úÖ Pipeline Complete (Duration: 1.06 seconds)      ‚îÇ
‚îÇ            ‚îî‚îÄ Status: SUCCESS                                 ‚îÇ
‚îÇ                                                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DAY N+1: Business Actions (08:00 - 17:00)                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                ‚îÇ
‚îÇ  08:00  Procurement team reviews reports                      ‚îÇ
‚îÇ         ‚Ä¢ Check exception alerts                              ‚îÇ
‚îÇ         ‚Ä¢ Review high-value orders                            ‚îÇ
‚îÇ         ‚Ä¢ Verify demand spikes                                ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  09:00  Send supplier orders                                  ‚îÇ
‚îÇ         ‚Ä¢ Email JSON files to suppliers                       ‚îÇ
‚îÇ         ‚Ä¢ Confirm order acceptance                            ‚îÇ
‚îÇ         ‚Ä¢ Track order status                                  ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  10:00  Handle exceptions                                     ‚îÇ
‚îÇ         ‚Ä¢ Contact sales for demand spikes                     ‚îÇ
‚îÇ         ‚Ä¢ Expedite critical stock items                       ‚îÇ
‚îÇ         ‚Ä¢ Resolve missing supplier issues                     ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  N+3    Receive supplier deliveries                           ‚îÇ
‚îÇ         ‚Ä¢ 3-day lead time typical                             ‚îÇ
‚îÇ         ‚Ä¢ Update warehouse stock                              ‚îÇ
‚îÇ         ‚Ä¢ Replenish store shelves                             ‚îÇ
‚îÇ                                                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÇ Project Structure

```
Big-data/
‚îú‚îÄ‚îÄ README.MD                          # Comprehensive project documentation
‚îú‚îÄ‚îÄ TODO.MD                            # Project roadmap & task tracking
‚îÇ
‚îî‚îÄ‚îÄ procurement-pipeline/
    ‚îÇ
    ‚îú‚îÄ‚îÄ docker-compose.yml             # 8-container infrastructure
    ‚îú‚îÄ‚îÄ requirements.txt               # Python dependencies
    ‚îú‚îÄ‚îÄ setup.ps1                      # Windows automated setup
    ‚îú‚îÄ‚îÄ setup.sh                       # Linux/macOS automated setup
    ‚îú‚îÄ‚îÄ .gitignore                     # Git ignore rules
    ‚îÇ
    ‚îú‚îÄ‚îÄ HDFS_REPLICATION.md            # HDFS 3-way replication docs
    ‚îú‚îÄ‚îÄ PROJECT_STATUS.md              # Current project status
    ‚îú‚îÄ‚îÄ SUMMARY_REPORT.md              # Pipeline execution summaries
    ‚îÇ
    ‚îú‚îÄ‚îÄ airflow/
    ‚îÇ   ‚îú‚îÄ‚îÄ dags/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ procurement_dag.py     # Main DAG (9 tasks, 22:00 schedule)
    ‚îÇ   ‚îî‚îÄ‚îÄ logs/                      # Airflow execution logs
    ‚îÇ       ‚îî‚îÄ‚îÄ dag_id=procurement_daily_pipeline/
    ‚îÇ
    ‚îú‚îÄ‚îÄ config/
    ‚îÇ   ‚îî‚îÄ‚îÄ trino-config/
    ‚îÇ       ‚îú‚îÄ‚îÄ config.properties      # Trino server config
    ‚îÇ       ‚îú‚îÄ‚îÄ jvm.config             # JVM memory settings
    ‚îÇ       ‚îú‚îÄ‚îÄ node.properties        # Node configuration
    ‚îÇ       ‚îî‚îÄ‚îÄ catalog/
    ‚îÇ           ‚îú‚îÄ‚îÄ hive.properties    # HDFS catalog
    ‚îÇ           ‚îî‚îÄ‚îÄ postgresql.properties  # PostgreSQL catalog
    ‚îÇ
    ‚îú‚îÄ‚îÄ data/
    ‚îÇ   ‚îú‚îÄ‚îÄ raw/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ orders/                # 105 JSON files (15 stores √ó 7 days)
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sample_pos_1_2026-01-03.json
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stock/                 # 35 CSV files (5 warehouses √ó 7 days)
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ sample_warehouse_1_2026-01-03.csv
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ...
    ‚îÇ   ‚îî‚îÄ‚îÄ output/
    ‚îÇ       ‚îú‚îÄ‚îÄ replenishment_*.csv            # Net demand calculations
    ‚îÇ       ‚îú‚îÄ‚îÄ supplier_orders/*.json         # Supplier order files
    ‚îÇ       ‚îú‚îÄ‚îÄ exception_report_*.json        # Anomaly alerts
    ‚îÇ       ‚îî‚îÄ‚îÄ pipeline_summary_*.txt         # Execution summaries
    ‚îÇ
    ‚îú‚îÄ‚îÄ logs/                          # Application logs
    ‚îÇ
    ‚îú‚îÄ‚îÄ scripts/
    ‚îÇ   ‚îú‚îÄ‚îÄ data_gen.py                # Generate synthetic test data
    ‚îÇ   ‚îú‚îÄ‚îÄ ingest_hdfs.py             # Upload files to HDFS
    ‚îÇ   ‚îú‚îÄ‚îÄ compute_demand.py          # Calculate net demand (Trino)
    ‚îÇ   ‚îú‚îÄ‚îÄ export_orders.py           # Generate supplier orders
    ‚îÇ   ‚îú‚îÄ‚îÄ generate_exceptions.py     # Detect anomalies
    ‚îÇ   ‚îú‚îÄ‚îÄ validate_data_quality.py   # Data quality checks
    ‚îÇ   ‚îú‚îÄ‚îÄ test_connection.py         # Test Trino/PostgreSQL connectivity
    ‚îÇ   ‚îú‚îÄ‚îÄ test_system.py             # System validation tests
    ‚îÇ   ‚îú‚îÄ‚îÄ run_pipeline.py            # Manual pipeline execution
    ‚îÇ   ‚îî‚îÄ‚îÄ upload_to_hdfs.ps1         # PowerShell HDFS upload
    ‚îÇ
    ‚îî‚îÄ‚îÄ sql/
        ‚îú‚îÄ‚îÄ postgres/
        ‚îÇ   ‚îú‚îÄ‚îÄ schema.sql             # Database schema
        ‚îÇ   ‚îú‚îÄ‚îÄ init_master_data.sql   # Load products & suppliers
        ‚îÇ   ‚îî‚îÄ‚îÄ 02_create_airflow_db.sql  # Airflow metadata DB
        ‚îî‚îÄ‚îÄ presto/
            ‚îî‚îÄ‚îÄ analysis.sql           # Example Trino queries
```

---

## Usage

### Daily Operations

#### Start the System

```powershell
# Navigate to project directory
cd procurement-pipeline

# Start all containers
docker-compose up -d

# Verify all services are running
docker ps

# Check logs
docker-compose logs -f --tail=50
```

#### Access Web Interfaces

| Service | URL | Credentials | Purpose |
|---------|-----|-------------|---------|
| **Airflow** | http://localhost:8081 | admin/admin | Workflow monitoring & DAG execution |
| **Trino** | http://localhost:8080 | No auth | Query engine UI & cluster status |
| **HDFS** | http://localhost:9870 | No auth | File system browser & datanode status |
| **pgAdmin** | http://localhost:5050 | admin@admin.com/admin | PostgreSQL database management |

#### Manual Pipeline Execution

```powershell
# Run complete pipeline for specific date
docker exec procurement_airflow python /opt/airflow/scripts/run_pipeline.py --date 2026-01-03

# Run individual stages
docker exec procurement_airflow python /opt/airflow/scripts/compute_demand.py --date 2026-01-03
docker exec procurement_airflow python /opt/airflow/scripts/export_orders.py --date 2026-01-03
docker exec procurement_airflow python /opt/airflow/scripts/generate_exceptions.py --date 2026-01-03
```

#### Generate New Test Data

```powershell
# Generate data for specific date range
docker exec procurement_airflow python /opt/airflow/scripts/data_gen.py \
    --start-date 2026-01-04 \
    --end-date 2026-01-10 \
    --stores 15 \
    --warehouses 5

# Upload to HDFS
.\scripts\upload_to_hdfs.ps1
```

### Query Operations

#### Trino Query Examples

```sql
-- Connect to Trino
docker exec -it procurement_trino trino

-- View all catalogs
SHOW CATALOGS;

-- List HDFS tables
SHOW TABLES FROM hive.default;

-- Query order data
SELECT 
    sku,
    SUM(quantity) as total_quantity,
    COUNT(DISTINCT pos_id) as store_count
FROM hive.default.orders
WHERE order_date = DATE '2026-01-03'
GROUP BY sku
ORDER BY total_quantity DESC
LIMIT 10;

-- Query stock levels
SELECT 
    warehouse_id,
    sku,
    quantity
FROM hive.default.stock
WHERE stock_date = DATE '2026-01-03'
ORDER BY warehouse_id, sku;

-- Federated query: Join HDFS + PostgreSQL
SELECT 
    o.sku,
    p.product_name,
    p.category,
    SUM(o.quantity) as demand
FROM hive.default.orders o
JOIN postgresql.procurement.products p ON o.sku = p.sku
WHERE o.order_date = DATE '2026-01-03'
GROUP BY o.sku, p.product_name, p.category
ORDER BY demand DESC;
```

#### PostgreSQL Query Examples

```bash
# Connect to PostgreSQL
docker exec -it procurement_postgres psql -U admin -d procurement_db

# List all tables
\dt

# View products
SELECT * FROM products LIMIT 10;

# View suppliers
SELECT supplier_name, contact_email, COUNT(*) as product_count
FROM products
GROUP BY supplier_name, contact_email
ORDER BY product_count DESC;

# View replenishment rules
SELECT 
    product_id,
    product_name,
    moq,
    case_size,
    lead_time_days
FROM products
WHERE category = 'Dairy';
```

### HDFS Operations

#### File Management

```bash
# List HDFS directories
docker exec procurement_namenode hdfs dfs -ls /

# View raw data folders
docker exec procurement_namenode hdfs dfs -ls /raw/orders
docker exec procurement_namenode hdfs dfs -ls /raw/stock

# Check file content
docker exec procurement_namenode hdfs dfs -cat /raw/orders/sample_pos_1_2026-01-03.json

# Get file statistics
docker exec procurement_namenode hdfs dfs -stat "%r %b %n" /raw/orders/sample_pos_1_2026-01-03.json

# Remove file
docker exec procurement_namenode hdfs dfs -rm /path/to/file

# Upload file from local to HDFS
docker exec procurement_namenode hdfs dfs -put /local/path/file.json /hdfs/path/
```

#### Cluster Health Checks

```bash
# Full cluster report
docker exec procurement_namenode hdfs dfsadmin -report

# Check replication status
docker exec procurement_namenode hdfs fsck / -files -blocks -locations

# Verify 3-way replication
docker exec procurement_namenode hdfs fsck / -files -blocks | grep -E "replication="

# Datanode status
docker exec procurement_namenode hdfs dfsadmin -printTopology
```

### Airflow Operations

#### DAG Management

```bash
# List all DAGs
docker exec procurement_airflow airflow dags list

# Trigger manual run
docker exec procurement_airflow airflow dags trigger procurement_daily_pipeline

# Trigger with specific execution date
docker exec procurement_airflow airflow dags trigger procurement_daily_pipeline \
    --exec-date 2026-01-03

# Pause DAG
docker exec procurement_airflow airflow dags pause procurement_daily_pipeline

# Unpause DAG
docker exec procurement_airflow airflow dags unpause procurement_daily_pipeline
```

#### Task Management

```bash
# List tasks in DAG
docker exec procurement_airflow airflow tasks list procurement_daily_pipeline

# Test specific task
docker exec procurement_airflow airflow tasks test \
    procurement_daily_pipeline \
    compute_demand \
    2026-01-03

# View task logs
docker exec procurement_airflow cat /opt/airflow/logs/dag_id=procurement_daily_pipeline/run_id=manual__2026-01-03/task_id=compute_demand/attempt=1.log
```

---

## Monitoring

### System Health Dashboard

#### Container Status

```bash
# Check all containers
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

# Expected output:
# NAMES                      STATUS              PORTS
# procurement_airflow        Up 2 hours          0.0.0.0:8081->8080/tcp
# procurement_trino          Up 2 hours          0.0.0.0:8080->8080/tcp
# procurement_namenode       Up 2 hours          0.0.0.0:9000->9000/tcp, 0.0.0.0:9870->9870/tcp
# procurement_datanode1      Up 2 hours          0.0.0.0:9864->9864/tcp
# procurement_datanode2      Up 2 hours          0.0.0.0:9865->9864/tcp
# procurement_datanode3      Up 2 hours          0.0.0.0:9866->9864/tcp
# procurement_postgres       Up 2 hours          0.0.0.0:5432->5432/tcp
# procurement_pgadmin        Up 2 hours          0.0.0.0:5050->80/tcp
```

#### Service Health Endpoints

```bash
# Trino health
curl http://localhost:8080/v1/info | jq

# HDFS namenode
curl http://localhost:9870/jmx | jq '.beans[] | select(.name=="Hadoop:service=NameNode,name=NameNodeStatus")'

# Airflow health
curl http://localhost:8081/health | jq

# PostgreSQL connection
docker exec procurement_postgres pg_isready -U admin
```

### Pipeline Monitoring

#### Airflow UI (Recommended)

1. Open http://localhost:8081
2. Login: `admin` / `admin`
3. View DAG: `procurement_daily_pipeline`
4. Check:
   - ‚úÖ **Graph View**: Visual task dependencies
   - ‚úÖ **Tree View**: Historical execution timeline
   - ‚úÖ **Gantt View**: Task duration analysis
   - ‚úÖ **Task Duration**: Performance trends
   - ‚úÖ **Logs**: Detailed execution output

#### Key Metrics to Monitor

| Metric | Healthy Threshold | Warning Threshold | Critical Threshold |
|--------|-------------------|-------------------|-------------------|
| **Pipeline Duration** | < 60s | 60-120s | > 120s |
| **Success Rate** | 100% | 95-99% | < 95% |
| **Data Files** | 140 files | 130-140 files | < 130 files |
| **HDFS Disk Usage** | < 50% | 50-80% | > 80% |
| **Exception Count** | < 10 | 10-30 | > 30 |
| **Critical Exceptions** | 0 | 1-2 | > 2 |

### Log Monitoring

```bash
# Real-time logs for all services
docker-compose logs -f

# Specific service logs
docker-compose logs -f airflow
docker-compose logs -f trino
docker-compose logs -f namenode

# Airflow task logs
docker exec procurement_airflow ls -lh /opt/airflow/logs/dag_id=procurement_daily_pipeline/

# View latest pipeline summary
docker exec procurement_airflow cat /opt/airflow/data/output/pipeline_summary_$(date +%Y-%m-%d).txt
```

---

## Troubleshooting

### Common Issues & Solutions

#### Issue: Containers Won't Start

**Symptom:**
```bash
docker ps
# Shows containers restarting or not running
```

**Solution:**
```powershell
# Check detailed logs
docker-compose logs --tail=100

# Common fixes:
# 1. Port conflict - Check if ports are in use
netstat -ano | findstr "5432 8080 8081 9000 9870"

# 2. Insufficient resources
# Increase Docker Desktop memory to 8GB+

# 3. Corrupted volumes - Reset
docker-compose down -v
docker-compose up -d

# 4. Network issues
docker network prune
docker-compose up -d
```

#### Issue: Trino Can't Connect to PostgreSQL

**Symptom:**
```
Error: Failed to connect to catalog 'postgresql'
```

**Solution:**
```bash
# 1. Verify PostgreSQL is running
docker exec procurement_postgres pg_isready -U admin

# 2. Test connection from Trino
docker exec -it procurement_trino trino --execute "SHOW CATALOGS"

# 3. Check catalog configuration
docker exec procurement_trino cat /etc/trino/catalog/postgresql.properties

# Should contain:
# connector.name=postgresql
# connection-url=jdbc:postgresql://postgres:5432/procurement_db
# connection-user=admin
# connection-password=password

# 4. Restart Trino if config was changed
docker-compose restart trino
```

#### Issue: HDFS Datanode Not Registering

**Symptom:**
```bash
hdfs dfsadmin -report
# Shows less than 4 datanodes
```

**Solution:**
```bash
# 1. Check datanode logs
docker-compose logs datanode1 datanode2 datanode3

# 2. Verify namenode is healthy
docker exec procurement_namenode hdfs dfsadmin -safemode get

# 3. If in safe mode, force leave
docker exec procurement_namenode hdfs dfsadmin -safemode leave

# 4. Restart datanodes
docker-compose restart datanode1 datanode2 datanode3

# 5. Wait 30 seconds and verify
docker exec procurement_namenode hdfs dfsadmin -report
```

#### Issue: Airflow DAG Not Appearing

**Symptom:**
```
DAG 'procurement_daily_pipeline' not found in UI
```

**Solution:**
```bash
# 1. Check DAG file exists
docker exec procurement_airflow ls -l /opt/airflow/dags/procurement_dag.py

# 2. Check for Python syntax errors
docker exec procurement_airflow python -m py_compile /opt/airflow/dags/procurement_dag.py

# 3. Force DAG refresh
docker exec procurement_airflow airflow dags list-import-errors

# 4. Restart Airflow
docker-compose restart airflow

# 5. Check Airflow logs
docker-compose logs airflow | grep "ERROR"
```

#### Issue: Pipeline Fails on Data Validation

**Symptom:**
```
Task 'data_quality_check' failed
```

**Solution:**
```bash
# 1. Check data files exist
docker exec procurement_airflow ls /opt/airflow/data/raw/orders/*.json | wc -l
docker exec procurement_airflow ls /opt/airflow/data/raw/stock/*.csv | wc -l

# 2. Validate file format
docker exec procurement_airflow cat /opt/airflow/data/raw/orders/sample_pos_1_2026-01-03.json | jq .

# 3. Run validation script manually
docker exec procurement_airflow python /opt/airflow/scripts/validate_data_quality.py

# 4. Check for missing fields
docker exec procurement_airflow grep -l "null" /opt/airflow/data/raw/orders/*.json
```

#### Issue: Trino Queries Timeout

**Symptom:**
```
Query exceeded maximum time limit of 30s
```

**Solution:**
```bash
# 1. Check Trino resource usage
curl http://localhost:8080/v1/info | jq '.memoryInfo'

# 2. Increase query timeout in config
docker exec procurement_trino cat /etc/trino/config.properties
# Add: query.max-execution-time=10m

# 3. Optimize query (use filters early)
# ‚ùå Bad: SELECT * FROM hive.default.orders WHERE sku = 'SKU-0001'
# ‚úÖ Good: SELECT sku, quantity FROM hive.default.orders WHERE order_date = DATE '2026-01-03' AND sku = 'SKU-0001'

# 4. Restart Trino after config change
docker-compose restart trino
```

### Quick Diagnostic Commands

```bash
# System Overview
docker ps --format "table {{.Names}}\t{{.Status}}"
docker stats --no-stream

# Service Connectivity Test
docker exec procurement_airflow python /opt/airflow/scripts/test_connection.py

# HDFS Health Check
docker exec procurement_namenode hdfs fsck / | grep -E "Status|Total"

# PostgreSQL Health Check
docker exec procurement_postgres psql -U admin -d procurement_db -c "SELECT COUNT(*) FROM products"

# Trino Health Check
curl -s http://localhost:8080/v1/info | jq '.starting'

# Airflow Scheduler Status
docker exec procurement_airflow airflow jobs check --job-type SchedulerJob

# View Latest Exceptions
docker exec procurement_airflow cat /opt/airflow/data/output/exception_report_$(date +%Y-%m-%d).json | jq '.summary'
```

### Emergency Reset
    ‚îÇ
    ‚îú‚îÄ‚îÄ docker-compose.yml             # Infrastructure definition
    ‚îÇ
    ‚îú‚îÄ‚îÄ airflow/
    ‚îÇ   ‚îú‚îÄ‚îÄ dags/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ procurement_dag.py     # Airflow DAG (22:00 schedule)
    ‚îÇ   ‚îî‚îÄ‚îÄ logs/                      # Airflow execution logs
    ‚îÇ
    ‚îú‚îÄ‚îÄ config/
    ‚îÇ   ‚îú‚îÄ‚îÄ trino/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ postgresql.properties  # Trino PostgreSQL catalog
    ‚îÇ   ‚îî‚îÄ‚îÄ trino-config/
    ‚îÇ       ‚îî‚îÄ‚îÄ config.properties      # Trino server config
    ‚îÇ
    ‚îú‚îÄ‚îÄ data/
    ‚îÇ   ‚îú‚îÄ‚îÄ raw/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ orders/                # Daily POS order files (JSON)
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pos_1_2025-12-28.json
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pos_1_2025-12-29.json
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (105 files: 15 POS √ó 7 days)
    ‚îÇ   ‚îÇ   ‚îÇ
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stock/                 # Daily warehouse snapshots (CSV)
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ warehouse_1_2025-12-28.csv
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ... (35 files: 5 warehouses √ó 7 days)
    ‚îÇ   ‚îÇ
    ‚îÇ   ‚îî‚îÄ‚îÄ output/
    ‚îÇ       ‚îú‚îÄ‚îÄ replenishment_2026-01-03.csv
    ‚îÇ       ‚îú‚îÄ‚îÄ pipeline_run_2026-01-03.txt
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ‚îÄ supplier_orders/       # JSON orders per supplier
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ BevCo_Distributors_2026-01-03.json
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ DairyFresh_LLC_2026-01-03.json
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ ElectroSupply_Co_2026-01-03.json
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ FreshMart_Wholesale_2026-01-03.json
    ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ TechGear_Plus_2026-01-03.json
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îî‚îÄ‚îÄ exceptions/            # Quality control reports
    ‚îÇ           ‚îú‚îÄ‚îÄ exception_report_2026-01-03.json
    ‚îÇ           ‚îî‚îÄ‚îÄ exception_summary_2026-01-03.txt
    ‚îÇ
    ‚îú‚îÄ‚îÄ scripts/
    ‚îÇ   ‚îú‚îÄ‚îÄ data_gen.py                # Faker-based test data generator
    ‚îÇ   ‚îú‚îÄ‚îÄ generate_orders.py         # Order generation script
    ‚îÇ   ‚îú‚îÄ‚îÄ ingest_hdfs.py             # HDFS data ingestion
    ‚îÇ   ‚îú‚îÄ‚îÄ upload_to_hdfs.ps1         # PowerShell upload script
    ‚îÇ   ‚îú‚îÄ‚îÄ validate_data_quality.py   # Data quality validation
    ‚îÇ   ‚îú‚îÄ‚îÄ compute_demand.py          # Main demand calculation
    ‚îÇ   ‚îú‚îÄ‚îÄ export_orders.py           # Supplier JSON generation
    ‚îÇ   ‚îú‚îÄ‚îÄ generate_exceptions.py     # Anomaly detection & reporting
    ‚îÇ   ‚îú‚îÄ‚îÄ run_phase4.py              # Phase 4 orchestration
    ‚îÇ   ‚îú‚îÄ‚îÄ run_pipeline.py            # Master pipeline orchestrator
    ‚îÇ   ‚îú‚îÄ‚îÄ test_connection.py         # Infrastructure connectivity test
    ‚îÇ   ‚îú‚îÄ‚îÄ test_trino_catalog.py      # Trino catalog validation
    ‚îÇ   ‚îî‚îÄ‚îÄ test_system.py             # Full system integration test
    ‚îÇ
    ‚îî‚îÄ‚îÄ sql/
        ‚îî‚îÄ‚îÄ postgres/
            ‚îú‚îÄ‚îÄ schema.sql             # Database schema
            ‚îî‚îÄ‚îÄ init_master_data.sql   # Seed data (products, suppliers)
```

---

## Advanced Setup

### Prerequisites

- Docker Desktop installed and running
- Python 3.10+ with pip
- Git

### 1. Clone Repository

```bash
git clone https://github.com/mohamedamineelabidi/Big-data.git
cd Big-data/procurement-pipeline
```

### 2. Start Infrastructure

```bash
docker-compose up -d
```

### 3. Install Python Dependencies

```bash
pip install pandas trino psycopg2-binary faker
```

### 4. Verify Services

```bash
python scripts/test_connection.py
```

### 5. Run Full Pipeline

```bash
# Run for specific date
python scripts/run_pipeline.py --date 2026-01-03

# Or run system test
python scripts/test_system.py
```

### 6. Access Web Interfaces

| Service | URL | Credentials |
|---------|-----|-------------|
| **Airflow UI** | http://localhost:8081 | admin / admin |
| **Trino UI** | http://localhost:8080 | admin (no password) |
| **pgAdmin** | http://localhost:5050 | admin@admin.com / admin |
| **HDFS UI** | http://localhost:9870 | No auth required |

---

## üìú Scripts Documentation

### Core Pipeline Scripts

#### **1. run_pipeline.py** - Master Orchestrator

**Purpose:** Executes the complete procurement pipeline with all stages

**Key Features:**
- Infrastructure validation
- Stage-by-stage execution with error handling
- Performance metrics tracking
- Historical replay capability
- Detailed logging and reporting

**Usage:**
```bash
# Run for specific date
python scripts/run_pipeline.py --date 2026-01-03

# Replay last N days
python scripts/run_pipeline.py --replay 7

# Skip validation stage (faster)
python scripts/run_pipeline.py --skip-validation

# Validation only (no processing)
python scripts/run_pipeline.py --validate-only
```

**Output Files:**
- `data/output/pipeline_run_YYYY-MM-DD.txt` - Execution summary

---

#### **2. compute_demand.py** - Demand Calculation Engine

**Purpose:** Core analytics - calculates what products to reorder

**Algorithm:**
```
1. Load orders from HDFS (via Trino Hive catalog)
2. Load stock from HDFS (via Trino Hive catalog)
3. Load master data from PostgreSQL (via Trino PostgreSQL catalog)
4. Aggregate demand per SKU
5. Calculate: Net Demand = Orders - Stock + Safety Stock
6. Apply business rules (MOQ, case size, priorities)
7. Save replenishment CSV
```

**Usage:**
```bash
python scripts/compute_demand.py --date 2026-01-03
```

**Output Files:**
- `data/output/replenishment_YYYY-MM-DD.csv` - Procurement recommendations

**Performance:**
- Processes 15,150 order items in ~500ms
- Handles 50+ unique SKUs
- Memory efficient (< 100 MB)

---

#### **3. export_orders.py** - Supplier Order Generator

**Purpose:** Transforms replenishment CSV into supplier-specific JSON orders

**Logic:**
```python
1. Load replenishment_YYYY-MM-DD.csv
2. Group items by supplier_name
3. For each supplier:
   - Create order JSON structure
   - Calculate totals (items, units, cases, value)
   - Set delivery dates (current_date + lead_time)
   - Assign order priorities
4. Save JSON file per supplier
```

**Usage:**
```bash
python scripts/export_orders.py --date 2026-01-03
```

**Output Files:**
- `data/output/supplier_orders/SupplierName_YYYY-MM-DD.json` (5 files)

---

#### **4. generate_exceptions.py** - Anomaly Detection

**Purpose:** Detect business rule violations and unusual patterns

**Detection Rules:**
```python
HIGH_DEMAND:
  - Threshold: > 2,000 units
  - Check: Compare to historical average
  - Alert: "Unusual spike - verify before ordering"

LOW_STOCK:
  - Threshold: stock < 30% of demand
  - Check: Stock coverage ratio
  - Alert: "Low inventory - risk of stockout"

CRITICAL_STOCK:
  - Threshold: stock < 10% of demand
  - Check: Critical coverage
  - Alert: "URGENT - emergency order required"

MISSING_SUPPLIER:
  - Check: supplier_name IS NULL
  - Alert: "Cannot generate order - assign supplier"

HIGH_VALUE_ORDER:
  - Threshold: order_value > $10,000
  - Check: Total order cost
  - Alert: "Manager approval required"

DEMAND_STOCK_GAP:
  - Threshold: demand > 5x stock
  - Check: Demand-to-stock ratio
  - Alert: "Large gap - review forecasting"
```

**Usage:**
```bash
python scripts/generate_exceptions.py --date 2026-01-03
```

**Output Files:**
- `data/output/exceptions/exception_report_YYYY-MM-DD.json` - Detailed exceptions
- `data/output/exceptions/exception_summary_YYYY-MM-DD.txt` - Human-readable report

---

### Testing & Validation Scripts

#### **5. test_system.py** - Comprehensive Integration Tests

**Purpose:** Validates entire system with 37 automated tests

**Test Coverage:**
```
Docker Services (5 tests)
‚îú‚îÄ Container health checks
‚îú‚îÄ Port availability
‚îî‚îÄ Network connectivity

Database Layer (6 tests)
‚îú‚îÄ PostgreSQL connection
‚îú‚îÄ Table structure validation
‚îú‚îÄ Data integrity checks
‚îú‚îÄ Trino connectivity
‚îú‚îÄ Catalog availability
‚îî‚îÄ Query execution

Data Files (3 tests)
‚îú‚îÄ Order file count (105 expected)
‚îú‚îÄ Stock file count (35 expected)
‚îî‚îÄ File format validation

Processing Modules (12 tests)
‚îú‚îÄ compute_demand.py functionality
‚îú‚îÄ export_orders.py execution
‚îú‚îÄ generate_exceptions.py rules
‚îî‚îÄ Data transformation accuracy

Orchestration (7 tests)
‚îú‚îÄ run_pipeline.py execution
‚îú‚îÄ Airflow DAG syntax
‚îú‚îÄ Task dependencies
‚îî‚îÄ Schedule configuration

End-to-End (4 tests)
‚îú‚îÄ Full pipeline execution
‚îú‚îÄ Output file generation
‚îú‚îÄ Data consistency
‚îî‚îÄ Performance benchmarks
```

**Usage:**
```bash
python scripts/test_system.py

# Expected output:
# ‚úÖ 37/37 tests passed (100%)
# ‚è±Ô∏è  Total time: ~15 seconds
```

---

#### **6. test_trino_catalog.py** - Trino Connectivity Tests

**Purpose:** Validates Trino query engine and data access

**Test Categories:**
```
1. Version Check
   - Verify Trino 479 running

2. Catalog Discovery
   - List available catalogs (postgresql, hive, system)

3. Schema Exploration
   - List schemas in each catalog

4. Table Queries
   - Simple SELECT queries
   - COUNT aggregations
   - GROUP BY operations

5. JOIN Operations
   - Cross-catalog joins (PostgreSQL + Hive)
   - Multi-table joins

6. Complex Analytics
   - Subqueries
   - Window functions
   - Aggregations with filters
```

**Usage:**
```bash
python scripts/test_trino_catalog.py

# Expected: 10/10 tests passed
```

---

#### **7. test_connection.py** - Infrastructure Health Check

**Purpose:** Quick connectivity test for all services

**Checks:**
```
‚úì PostgreSQL: Connection + query execution
‚úì HDFS: Namenode accessibility + file system operations
‚úì Trino: Query engine + catalog availability
```

**Usage:**
```bash
python scripts/test_connection.py

# Expected output:
# ‚úÖ PostgreSQL Connected
# ‚úÖ HDFS Connected
# ‚úÖ Trino Connected
```

---

### Data Generation Scripts

#### **8. data_gen.py** - Test Data Generator

**Purpose:** Create realistic test data using Faker library

**Generated Data:**
```
Orders (JSON files):
‚îú‚îÄ 15 POS locations (pos_1 to pos_15)
‚îú‚îÄ 7 days of history (Dec 28 - Jan 03)
‚îú‚îÄ 100-150 orders per store per day
‚îú‚îÄ 10-15 items per order
‚îú‚îÄ Realistic product distributions
‚îî‚îÄ Total: 105 files, ~15,150 items

Stock (CSV files):
‚îú‚îÄ 5 warehouses (WH-001 to WH-005)
‚îú‚îÄ 7 days of snapshots
‚îú‚îÄ 50 SKUs per warehouse
‚îú‚îÄ Random stock levels (0-500 units)
‚îî‚îÄ Total: 35 files, ~250 records/file
```

**Usage:**
```bash
# Generate 7 days of data
python scripts/data_gen.py --days 7 --start-date 2025-12-28

# Generate with custom stores/warehouses
python scripts/data_gen.py --stores 20 --warehouses 10
```

---

#### **9. validate_data_quality.py** - Data Quality Validator

**Purpose:** Ensure data integrity before processing

**Validation Rules:**
```
JSON Orders:
‚úì Valid JSON structure
‚úì Required fields present (order_id, pos_id, items)
‚úì Data types correct (quantity = integer, price = decimal)
‚úì Logical constraints (quantity > 0, price > 0)
‚úì Date format validation

CSV Stock:
‚úì Valid CSV format
‚úì Required columns present
‚úì Numeric fields are numeric
‚úì No negative quantities
‚úì SKU references valid products
```

**Usage:**
```bash
python scripts/validate_data_quality.py --date 2026-01-03
```

---

### Utility Scripts

#### **10. ingest_hdfs.py** - HDFS Upload

**Purpose:** Upload data files to HDFS

**Usage:**
```bash
python scripts/ingest_hdfs.py --source data/raw --hdfs-path /procurement
```

---

### CLI Options Reference

| Script | Option | Description |
|--------|--------|-------------|
| run_pipeline.py | `--date YYYY-MM-DD` | Process specific date |
| | `--replay N` | Replay last N days |
| | `--skip-validation` | Skip data validation stage |
| | `--validate-only` | Only run validation |
| | `--output PATH` | Custom output directory |
| compute_demand.py | `--date YYYY-MM-DD` | Calculation date |
| | `--base-path PATH` | Data directory path |
| export_orders.py | `--date YYYY-MM-DD` | Order generation date |
| | `--supplier NAME` | Generate for specific supplier |
| data_gen.py | `--days N` | Number of days to generate |
| | `--start-date YYYY-MM-DD` | Start date for generation |
| | `--stores N` | Number of POS locations |
| | `--warehouses N` | Number of warehouses |

---

## üì¶ Data Models

### Input: Order JSON Schema

```json
{
  "pos_id": "POS-001",
  "date": "2026-01-03",
  "items": [
    {
      "sku": "SKU-0012",
      "quantity": 24,
      "unit_price": 2.99
    }
  ]
}
```

### Input: Stock CSV Schema

| Column | Type | Description |
|--------|------|-------------|
| warehouse_id | string | Warehouse identifier |
| sku | string | Product SKU |
| available_stock | integer | Units available |
| reserved_stock | integer | Units reserved |
| date | date | Snapshot date |

### Output: Supplier Order JSON

```json
{
  "order_id": "ORD-20260103-006-001",
  "supplier_id": "SUP-006",
  "supplier_name": "BevCo Distributors",
  "order_date": "2026-01-03",
  "requested_delivery_date": "2026-01-05",
  "status": "PENDING",
  "priority": "HIGH",
  "items": [
    {
      "line_number": 1,
      "sku": "SKU-0001",
      "product_name": "Organic Apple Juice",
      "category": "Beverages",
      "quantity_ordered": 1344,
      "cases": 56,
      "case_size": 24,
      "net_demand": 1333.0
    }
  ],
  "summary": {
    "total_line_items": 7,
    "total_units": 10494,
    "total_cases": 874
  },
  "metadata": {
    "generated_by": "procurement_pipeline",
    "generation_timestamp": "2026-01-04T03:52:28",
    "pipeline_version": "1.0"
  }
}
```

### Output: Exception Report

```json
{
  "report_date": "2026-01-03",
  "generated_at": "2026-01-04T02:58:07",
  "summary": {
    "total_skus_analyzed": 24,
    "total_exceptions": 30,
    "by_severity": {
      "CRITICAL": 0,
      "HIGH": 28,
      "MEDIUM": 2
    }
  },
  "exceptions": [
    {
      "type": "HIGH_DEMAND",
      "severity": "HIGH",
      "sku": "SKU-0022",
      "product_name": "Bagels 6pk",
      "metric_value": 2852,
      "threshold": 2000,
      "recommendation": "Consider expedited supplier contact"
    }
  ]
}
```

---

## Pipeline Results

### Latest Execution (2026-01-03)

```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                    PIPELINE EXECUTION SUMMARY                       ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  üìÖ Processing Date:     2026-01-03                                 ‚ïë
‚ïë  ‚è±Ô∏è  Execution Time:      1.06 seconds                              ‚ïë
‚ïë  üìä Status:              ‚úÖ SUCCESS                                 ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  üì¶ INPUT DATA                                                      ‚ïë
‚ïë     ‚Ä¢ Order Files:        105 JSON files                            ‚ïë
‚ïë     ‚Ä¢ Stock Files:        35 CSV files                              ‚ïë
‚ïë     ‚Ä¢ Order Items:        15,150 items                              ‚ïë
‚ïë     ‚Ä¢ Stock Records:      250 records                               ‚ïë
‚ïë     ‚Ä¢ POS Locations:      15 stores                                 ‚ïë
‚ïë     ‚Ä¢ Warehouses:         5 locations                               ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  üìä PROCESSING                                                      ‚ïë
‚ïë     ‚Ä¢ Total Demand:       121,840 units                             ‚ïë
‚ïë     ‚Ä¢ Total Stock:        58,401 units                              ‚ïë
‚ïë     ‚Ä¢ Unique SKUs:        50 products                               ‚ïë
‚ïë     ‚Ä¢ Products (Master):  49 in database                            ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  üì§ OUTPUT DATA                                                     ‚ïë
‚ïë     ‚Ä¢ SKUs to Reorder:    24 products                               ‚ïë
‚ïë     ‚Ä¢ Total Units:        32,748 units                              ‚ïë
‚ïë     ‚Ä¢ Suppliers:          5 orders generated                        ‚ïë
‚ïë     ‚Ä¢ Exceptions:         30 (0 critical)                           ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

### Supplier Orders Generated

| Supplier | SKUs | Units | Cases | Priority |
|----------|------|-------|-------|----------|
| BevCo Distributors | 7 | 10,494 | 874 | üî¥ HIGH |
| FreshMart Wholesale | 5 | 6,744 | 562 | üî¥ HIGH |
| DairyFresh LLC | 5 | 6,594 | 549 | üî¥ HIGH |
| ElectroSupply Co | 4 | 5,461 | 455 | üî¥ HIGH |
| TechGear Plus | 3 | 3,455 | 288 | üü° MEDIUM |
| **TOTAL** | **24** | **32,748** | **2,728** | |

### Top 10 Items to Reorder

| Rank | SKU | Product | Category | Order Qty | Supplier |
|------|-----|---------|----------|-----------|----------|
| 1 | SKU-0031 | USB Cable 2m | Electronics | 2,200 | ElectroSupply Co |
| 2 | SKU-0021 | Croissant Pack | Bakery | 2,040 | FreshMart Wholesale |
| 3 | SKU-0012 | Cola 2L | Beverages | 1,974 | BevCo Distributors |
| 4 | SKU-0005 | Almond Milk | Beverages | 1,728 | BevCo Distributors |
| 5 | SKU-0045 | Cream Cheese | Dairy | 1,692 | DairyFresh LLC |
| 6 | SKU-0024 | Baguette | Bakery | 1,540 | FreshMart Wholesale |
| 7 | SKU-0043 | Cheddar Cheese | Dairy | 1,510 | DairyFresh LLC |
| 8 | SKU-0003 | Wireless Mouse | Electronics | 1,440 | ElectroSupply Co |
| 9 | SKU-0022 | Bagels 6pk | Bakery | 1,424 | FreshMart Wholesale |
| 10 | SKU-0011 | Orange Juice 1L | Beverages | 1,416 | BevCo Distributors |

---

## ‚úÖ Project Phases Completed

### Phase 1: Infrastructure Setup ‚úÖ

- [x] Docker Compose with PostgreSQL, HDFS, Trino, pgAdmin, Airflow
- [x] PostgreSQL schema with products, suppliers, replenishment_rules
- [x] Master data: 49 products, 10 suppliers, 49 rules
- [x] All services verified and running

### Phase 2: Data Ingestion ‚úÖ

- [x] HDFS directory hierarchy (`/raw/orders/`, `/raw/stock/`)
- [x] 7 days of test data generated (Dec 28 - Jan 3)
- [x] 105 order files (15 POS √ó 7 days)
- [x] 35 stock files (5 warehouses √ó 7 days)
- [x] Data quality validation implemented

### Phase 3: Analytical Processing ‚úÖ

- [x] Trino connectors: PostgreSQL catalog, Hive catalog
- [x] Demand aggregation: 121,840 units across 50 SKUs
- [x] Net demand computation with safety stock
- [x] Business rules: MOQ and case size rounding
- [x] Result: 24 SKUs require 32,748 units

### Phase 4: Output Generation ‚úÖ

- [x] Supplier order export (5 JSON files)
- [x] Exception reporting (30 exceptions detected)
- [x] Order priorities assigned (4 HIGH, 1 MEDIUM)
- [x] Human-readable reports generated

### Phase 5: Orchestration ‚úÖ

- [x] Master orchestrator (`run_pipeline.py`)
- [x] Airflow DAG scheduled at 22:00 daily
- [x] Historical replay capability (`--replay N`)
- [x] System integration tests (37 tests, 100% pass)

---

## üß™ System Tests

Run the comprehensive test suite:

```bash
python scripts/test_system.py
```

### Test Categories

| Category | Tests | Description |
|----------|-------|-------------|
| Docker Services | 5 | Container health checks |
| PostgreSQL | 4 | Connection & data validation |
| Trino | 2 | Query engine verification |
| Data Files | 3 | File existence & counts |
| Demand Module | 4 | Data loading & processing |
| Export Module | 4 | JSON generation |
| Exception Module | 4 | Report generation |
| Orchestrator | 3 | Infrastructure validation |
| Airflow DAG | 4 | DAG syntax & configuration |
| End-to-End | 4 | Output file verification |
| **TOTAL** | **37** | **All passing ‚úÖ** |

---

## üê≥ Docker Services

```yaml
services:
  postgres:        # Master data storage
  namenode:        # HDFS name node
  datanode:        # HDFS data node
  trino:           # Query engine
  pgadmin:         # Database UI
  airflow:         # Workflow orchestration
```

### Service Ports

| Service | Internal Port | External Port |
|---------|---------------|---------------|
| PostgreSQL | 5432 | 5432 |
| HDFS Namenode | 9000, 9870 | 9000, 9870 |
| Trino | 8080 | 8080 |
| Airflow | 8080 | 8081 |
| pgAdmin | 80 | 5050 |

---

## Troubleshooting Guide

### Common Issues & Solutions

---

#### **1. Container Not Starting**

**Symptom:**
```
Error: Container exited with code 1
```

**Diagnosis:**
```bash
# Check container logs
docker logs procurement-trino
docker logs procurement-postgres

# Check port conflicts
netstat -ano | findstr "8080"  # Windows
netstat -tuln | grep 8080      # Linux
```

**Solutions:**
```bash
# 1. Port already in use
docker-compose down
# Kill process using the port
taskkill /PID <PID> /F  # Windows
kill -9 <PID>           # Linux
docker-compose up -d

# 2. Insufficient resources
docker system prune -a  # Free up space
# Increase Docker Desktop resources (RAM to 4GB+)

# 3. Corrupted volumes
docker-compose down -v
docker-compose up -d
```

---

#### **2. Trino Connection Failed**

**Symptom:**
```python
TrinoConnectionError: HTTP 503: Service Unavailable
```

**Diagnosis:**
```bash
# Check Trino health
curl http://localhost:8080/v1/info
docker exec -it procurement-trino trino --version

# Verify catalogs
docker exec -it procurement-trino ls /etc/trino/catalog
```

**Solutions:**
```bash
# 1. Trino not fully started (wait 30 seconds)
python -c "import time; print('Waiting...'); time.sleep(30)"
python scripts/test_connection.py

# 2. Catalog configuration error
# Check config/trino/postgresql.properties:
#   connector.name=postgresql
#   connection-url=jdbc:postgresql://postgres:5432/procurement
#   connection-user=admin
#   connection-password=admin123

# 3. Restart Trino
docker restart procurement-trino
```

---

#### **3. PostgreSQL Connection Error**

**Symptom:**
```
psycopg2.OperationalError: could not connect to server
```

**Diagnosis:**
```bash
# Test PostgreSQL directly
docker exec -it procurement-postgres psql -U admin -d procurement -c "SELECT 1;"

# Check PostgreSQL logs
docker logs procurement-postgres --tail 50
```

**Solutions:**
```bash
# 1. Database not initialized
docker exec -it procurement-postgres psql -U admin -d procurement
# If database doesn't exist:
docker exec -i procurement-postgres psql -U admin < sql/postgres/schema.sql
docker exec -i procurement-postgres psql -U admin -d procurement < sql/postgres/init_master_data.sql

# 2. Wrong credentials
# Update .env file or docker-compose.yml:
#   POSTGRES_USER=admin
#   POSTGRES_PASSWORD=admin123
#   POSTGRES_DB=procurement
```

---

#### **4. HDFS Permission Denied**

**Symptom:**
```
org.apache.hadoop.security.AccessControlException: Permission denied
```

**Diagnosis:**
```bash
# Check HDFS files
docker exec -it procurement-hdfs-namenode hdfs dfs -ls /procurement
docker exec -it procurement-hdfs-namenode hdfs dfs -ls /procurement/orders
```

**Solutions:**
```bash
# 1. Fix directory permissions
docker exec -it procurement-hdfs-namenode hdfs dfs -chmod -R 777 /procurement

# 2. Recreate HDFS directories
docker exec -it procurement-hdfs-namenode hdfs dfs -rm -r /procurement
docker exec -it procurement-hdfs-namenode hdfs dfs -mkdir -p /procurement/orders
docker exec -it procurement-hdfs-namenode hdfs dfs -mkdir -p /procurement/stock

# 3. Re-upload data
python scripts/ingest_hdfs.py --source data/raw --hdfs-path /procurement
```

---

#### **5. Airflow DAG Not Showing**

**Symptom:**
- DAG not visible in Airflow UI
- Import errors in logs

**Diagnosis:**
```bash
# Check DAG syntax
python airflow/dags/procurement_dag.py

# Check Airflow logs
docker logs procurement-airflow --tail 100
docker exec -it procurement-airflow airflow dags list
```

**Solutions:**
```bash
# 1. Python syntax error in DAG
# Fix syntax in airflow/dags/procurement_dag.py

# 2. Missing dependencies
docker exec -it procurement-airflow pip install pandas trino psycopg2-binary

# 3. Airflow database not created
docker exec procurement-postgres psql -U admin -d procurement -c "CREATE DATABASE airflow;"
docker restart procurement-airflow

# 4. Restart Airflow scheduler
docker restart procurement-airflow
```

---

#### **6. Pipeline Execution Failure**

**Symptom:**
```
ERROR: Stage 'compute_demand' failed
```

**Diagnosis:**
```bash
# Check pipeline logs
cat data/output/pipeline_run_2026-01-03.txt

# Check data files
ls data/raw/orders/*.json | wc -l  # Should be 105
ls data/raw/stock/*.csv | wc -l    # Should be 35
```

**Solutions:**
```bash
# 1. Missing data files
python scripts/data_gen.py --days 7 --start-date 2025-12-28
python scripts/ingest_hdfs.py

# 2. Run validation first
python scripts/run_pipeline.py --validate-only

# 3. Check specific stage
python scripts/compute_demand.py --date 2026-01-03
python scripts/export_orders.py --date 2026-01-03
```

---

#### **7. Test Failures**

**Symptom:**
```
‚ùå Test failed: test_docker_containers
```

**Diagnosis:**
```bash
# Run specific test
python scripts/test_system.py

# Check which containers are down
docker ps -a
```

**Solutions:**
```bash
# 1. Start all containers
docker-compose up -d

# 2. Wait for services to be ready
python scripts/test_connection.py

# 3. Re-run tests
python scripts/test_system.py
```

---

#### **8. Performance Issues**

**Symptom:**
- Pipeline takes > 5 seconds
- High memory usage

**Diagnosis:**
```bash
# Check Docker resource usage
docker stats

# Check Trino query performance
curl http://localhost:8080/v1/query
```

**Solutions:**
```bash
# 1. Increase Docker resources
# Docker Desktop ‚Üí Settings ‚Üí Resources
# - CPUs: 4+
# - Memory: 4GB+
# - Swap: 2GB+

# 2. Optimize Trino queries
# Add indexes in PostgreSQL:
CREATE INDEX idx_products_sku ON products(product_id);
CREATE INDEX idx_suppliers_name ON suppliers(supplier_name);

# 3. Process in batches
python scripts/run_pipeline.py --date 2026-01-03  # One day at a time
```

---

#### **9. Data Quality Issues**

**Symptom:**
```
WARNING: Invalid data detected in pos_1_2026-01-03.json
```

**Diagnosis:**
```bash
# Validate data files
python scripts/validate_data_quality.py --date 2026-01-03

# Check file contents
cat data/raw/orders/pos_1_2026-01-03.json | python -m json.tool
```

**Solutions:**
```bash
# 1. Regenerate corrupt files
python scripts/data_gen.py --days 1 --start-date 2026-01-03

# 2. Manual fix
# Edit JSON file to ensure valid structure:
# - All brackets closed
# - No trailing commas
# - Valid date formats

# 3. Skip validation (not recommended)
python scripts/run_pipeline.py --skip-validation
```

---

### Error Code Reference

| Error Code | Meaning | Action |
|------------|---------|--------|
| **HTTP 503** | Service Unavailable | Wait 30 seconds, check container health |
| **HTTP 404** | Catalog Not Found | Verify Trino catalog config files |
| **Exit Code 1** | Container Startup Failed | Check logs: `docker logs <container>` |
| **Exit Code 137** | Out of Memory | Increase Docker memory limit |
| **ECONNREFUSED** | Connection Refused | Check if service is running |
| **ETIMEDOUT** | Connection Timeout | Check network, increase timeout |

---

### Quick Diagnostics Commands

```bash
# System Health Check
python scripts/test_connection.py

# Container Status
docker-compose ps

# View All Logs
docker-compose logs --tail=50

# Resource Usage
docker stats --no-stream

# Database Connectivity
docker exec -it procurement-postgres psql -U admin -d procurement -c "\dt"

# HDFS Status
docker exec -it procurement-hdfs-namenode hdfs dfsadmin -report

# Trino Catalogs
docker exec -it procurement-trino trino --execute "SHOW CATALOGS"

# Full System Test
python scripts/test_system.py
```

---

### Support & Resources

**Official Documentation:**
- Trino: https://trino.io/docs/current/
- HDFS: https://hadoop.apache.org/docs/stable/
- Airflow: https://airflow.apache.org/docs/
- PostgreSQL: https://www.postgresql.org/docs/

**Project Resources:**
- Check logs in `airflow/logs/`
- Check pipeline output in `data/output/`
- Review test results from `test_system.py`

**Contact:**
- Project Repository: https://github.com/mohamedamineelabidi/Big-data
- Report bugs via GitHub Issues

---

## ü§ñ AI Development Context (GitHub Copilot)

### Project Understanding for AI Assistants

When providing assistance for this codebase, AI tools should understand:

---

#### **Domain Context:**

**Business Domain:** Retail Supply Chain Management & Procurement Automation

**Problem Statement:**
- Retail chain with 15 stores generates 15,150 daily transactions
- 5 warehouses manage inventory across 50 SKUs
- Need automated daily procurement to prevent stockouts
- Must consider: demand volatility, supplier lead times, MOQ constraints, case pack sizes

**Key Stakeholders:**
- **Store Managers:** Need consistent stock availability
- **Procurement Team:** Need accurate daily orders
- **Warehouse Managers:** Need optimal stock levels
- **Finance:** Need cost-effective ordering (MOQ compliance)

---

#### **Technical Architecture:**

**Data Engineering Pattern:** Batch-oriented ELT (Extract-Load-Transform)

**Technology Stack Rationale:**
```
HDFS (Data Lake)
  ‚Üì Stores: Raw JSON orders + CSV stock snapshots
  ‚Üì Why: Fault-tolerant, scalable for growing data volumes

Trino (Query Engine)
  ‚Üì Federates: PostgreSQL (master data) + Hive (HDFS data)
  ‚Üì Why: Single SQL interface for heterogeneous sources

PostgreSQL (Operational Database)
  ‚Üì Stores: Products, suppliers, business rules
  ‚Üì Why: ACID compliance for master data integrity

Airflow (Orchestrator)
  ‚Üì Schedules: Daily 22:00 execution
  ‚Üì Why: Visual DAG monitoring + error handling
```

**Not Used (Anti-patterns):**
- ‚ùå No Spark - Data volume too small (<1GB)
- ‚ùå No Kafka - Batch processing, not streaming
- ‚ùå No Cassandra - No high-write throughput needs
- ‚ùå No MongoDB - Structured data with relationships

---

#### **Code Conventions:**

**Variable Naming Patterns:**
```python
# Business Terms (Preferred)
sku, moq, safety_stock, net_demand, lead_time

# Data Sources
orders_df, stock_df, products_df, suppliers_df

# Processing Stages
raw_demand, adjusted_demand, rounded_demand

# Avoid Generic Names
data, temp, result, output
```

**Module Organization:**
```
scripts/
‚îú‚îÄ‚îÄ run_pipeline.py      # Orchestrator only - no business logic
‚îú‚îÄ‚îÄ compute_demand.py    # Core analytics - demand calculation
‚îú‚îÄ‚îÄ export_orders.py     # Output formatting - JSON generation
‚îú‚îÄ‚îÄ generate_exceptions.py  # Quality checks - anomaly detection
‚îî‚îÄ‚îÄ test_*.py            # Testing modules
```

**SQL Query Patterns:**
```sql
-- Always use explicit catalog.schema.table
SELECT * FROM postgresql.procurement.products

-- Always qualify joins
FROM postgresql.procurement.products p
JOIN hive.default.orders o ON p.product_id = o.sku

-- Always use meaningful aliases
SELECT p.product_name AS product, s.supplier_name AS supplier
```

---

#### **Common Development Tasks:**

**Adding a New Business Rule:**
```python
# 1. Update PostgreSQL schema
ALTER TABLE replenishment_rules ADD COLUMN new_rule_column INT;

# 2. Update init_master_data.sql
INSERT INTO replenishment_rules (product_id, new_rule_column) VALUES ...

# 3. Modify compute_demand.py
df['calculated_value'] = df.apply(lambda x: x['demand'] * x['new_rule_column'], axis=1)

# 4. Add test in test_system.py
def test_new_rule():
    assert calculated_value > 0
```

**Adding a New Exception Type:**
```python
# In generate_exceptions.py
if condition_met:
    exceptions.append({
        "exception_type": "NEW_EXCEPTION_TYPE",
        "severity": "HIGH",
        "sku": sku,
        "message": "Descriptive message",
        "action_required": "What to do"
    })
```

**Modifying Data Schema:**
```python
# 1. Update JSON schema in data_gen.py
order_data = {
    "pos_id": pos_id,
    "date": date,
    "new_field": value,  # Add here
    "items": [...]
}

# 2. Update compute_demand.py to handle new field
orders_df['new_field'] = orders_df['metadata'].apply(lambda x: x.get('new_field'))

# 3. Update test_system.py
def test_new_field_exists():
    with open('data/raw/orders/pos_1_2026-01-03.json') as f:
        data = json.load(f)
        assert 'new_field' in data
```

---

#### **Debugging Strategies:**

**For Query Errors:**
```python
# 1. Test Trino connectivity first
python scripts/test_connection.py

# 2. Run query directly in Trino CLI
docker exec -it procurement-trino trino --execute "SELECT * FROM postgresql.procurement.products LIMIT 5"

# 3. Check catalog configuration
cat config/trino/postgresql.properties

# 4. Verify table exists in PostgreSQL
docker exec -it procurement-postgres psql -U admin -d procurement -c "\dt"
```

**For Pipeline Failures:**
```python
# 1. Run validation only
python scripts/run_pipeline.py --validate-only

# 2. Run individual stages
python scripts/compute_demand.py --date 2026-01-03
python scripts/export_orders.py --date 2026-01-03

# 3. Check output logs
cat data/output/pipeline_run_2026-01-03.txt

# 4. Validate data quality
python scripts/validate_data_quality.py --date 2026-01-03
```

**For Test Failures:**
```python
# 1. Check container health
docker ps -a
docker-compose logs --tail=50

# 2. Verify data files exist
ls data/raw/orders/*.json | wc -l  # Should be 105
ls data/raw/stock/*.csv | wc -l    # Should be 35

# 3. Run individual test categories
python scripts/test_system.py  # See which tests fail

# 4. Check service accessibility
curl http://localhost:8080/v1/info  # Trino
docker exec -it procurement-postgres psql -U admin -d procurement -c "SELECT 1"  # PostgreSQL
```

---

#### **Performance Optimization Guidelines:**

**Query Optimization:**
```sql
-- ‚úÖ Good: Use filters early
SELECT * FROM hive.default.orders 
WHERE order_date = '2026-01-03'  -- Filter first
  AND quantity > 0

-- ‚ùå Bad: Filter after full table scan
SELECT * FROM hive.default.orders 
WHERE quantity > 0 AND order_date = '2026-01-03'
```

**DataFrame Operations:**
```python
# ‚úÖ Good: Vectorized operations
df['net_demand'] = df['demand'] - df['stock'] + df['safety_stock']

# ‚ùå Bad: Row-by-row iteration
for index, row in df.iterrows():
    df.at[index, 'net_demand'] = row['demand'] - row['stock'] + row['safety_stock']
```

**Memory Management:**
```python
# ‚úÖ Good: Process in chunks
for chunk in pd.read_csv('large_file.csv', chunksize=10000):
    process(chunk)

# ‚ùå Bad: Load entire file
df = pd.read_csv('large_file.csv')  # May cause OOM
```

---

#### **Data Validation Rules:**

**Order Files (JSON):**
```python
# Required fields
["pos_id", "date", "items"]

# Item structure
{
    "sku": "SKU-XXXX",      # Must match products table
    "quantity": int,         # Must be > 0
    "unit_price": float      # Must be > 0
}

# Date format
"YYYY-MM-DD"

# File naming
"pos_{1-15}_YYYY-MM-DD.json"
```

**Stock Files (CSV):**
```python
# Required columns
["warehouse_id", "sku", "date", "quantity"]

# Constraints
warehouse_id: "WH-001" to "WH-005"
sku: Must exist in products table
quantity: >= 0 (can be zero for out-of-stock)
date: YYYY-MM-DD format

# File naming
"stock_WH-XXX_YYYY-MM-DD.csv"
```

**Master Data (PostgreSQL):**
```sql
-- Products table
product_id: PRIMARY KEY, format "SKU-XXXX"
category: NOT NULL
supplier_name: FOREIGN KEY to suppliers

-- Replenishment Rules
moq: > 0 (Minimum Order Quantity)
case_size: > 0 (Units per case)
safety_stock: >= 0 (Can be zero)
lead_time: > 0 (Days)
```

---

#### **Testing Philosophy:**

**Unit Tests:** Individual module functionality
```python
# Test compute_demand.py in isolation
def test_demand_calculation():
    result = compute_demand(date='2026-01-03')
    assert len(result) > 0
    assert result['net_demand'].sum() > 0
```

**Integration Tests:** Module interactions
```python
# Test compute_demand ‚Üí export_orders pipeline
def test_full_chain():
    compute_demand(date='2026-01-03')
    export_orders(date='2026-01-03')
    assert os.path.exists('data/output/supplier_orders/BevCo_Distributors_2026-01-03.json')
```

**End-to-End Tests:** Full system validation
```python
# Test complete pipeline execution
def test_pipeline_execution():
    result = subprocess.run(['python', 'scripts/run_pipeline.py', '--date', '2026-01-03'])
    assert result.returncode == 0
    assert os.path.exists('data/output/replenishment_2026-01-03.csv')
```

---

#### **Deployment Considerations:**

**Environment Variables:**
```bash
# Development
POSTGRES_HOST=localhost
TRINO_HOST=localhost

# Production (would use)
POSTGRES_HOST=postgres.internal.company.com
TRINO_HOST=trino-cluster.internal.company.com
```

**Scaling Strategy:**
```
Current: 15 stores, 5 warehouses, 50 SKUs
‚Üì 
Scale to: 100 stores, 20 warehouses, 500 SKUs
‚Üì
Changes needed:
1. Increase HDFS datanodes (3 ‚Üí 10)
2. Add Trino workers (single node ‚Üí cluster)
3. Partition HDFS data by date (improve query performance)
4. Add caching layer (Redis) for master data
```

**Monitoring Metrics:**
```python
# Pipeline health
- Execution time (target: < 2 seconds)
- Success rate (target: 99.9%)
- Data freshness (target: < 24 hours old)

# Data quality
- Order file count (expected: 105)
- SKU coverage (expected: 50)
- Exception rate (target: < 5%)

# Resource usage
- Trino query time (target: < 500ms)
- PostgreSQL connections (max: 100)
- HDFS disk usage (target: < 80%)
```

---

#### **Common Pitfalls:**

**Trino Catalog Confusion:**
```sql
-- ‚ùå Wrong: Assuming catalog
SELECT * FROM products

-- ‚úÖ Correct: Explicit catalog
SELECT * FROM postgresql.procurement.products
```

**Date Handling:**
```python
# ‚ùå Wrong: String comparison
if date > "2026-01-03":

# ‚úÖ Correct: Date objects
from datetime import datetime
if datetime.strptime(date, '%Y-%m-%d') > datetime(2026, 1, 3):
```

**File Path Issues:**
```python
# ‚ùå Wrong: Hardcoded paths
file_path = "C:/Users/hp/Desktop/data/orders/..."

# ‚úÖ Correct: Relative paths
import os
base_path = os.path.dirname(__file__)
file_path = os.path.join(base_path, "../data/orders/...")
```

**Docker Container Names:**
```bash
# ‚ùå Wrong: Auto-generated names
docker logs big-data_postgres_1

# ‚úÖ Correct: Explicit names in docker-compose.yml
docker logs procurement-postgres
```

---

### Key Takeaways for AI Assistance:

1. **This is a DATA ENGINEERING project, not a web application**
2. **Batch processing only - no real-time/streaming requirements**
3. **Always use `trino` Python client, never `prestodb`**
4. **Explicit catalog names in all Trino queries**
5. **Business terminology over generic variable names**
6. **Test changes with `test_system.py` before committing**
7. **Master data lives in PostgreSQL, raw data in HDFS**
8. **Pipeline runs daily at 22:00, processing previous day's data**

---

### Emergency Reset

```powershell
# WARNING: This will DELETE all data and containers!

# Stop and remove all containers + volumes
docker-compose down -v

# Remove all project containers
docker rm -f $(docker ps -aq --filter "name=procurement")

# Remove networks
docker network prune -f

# Clean up volumes
docker volume prune -f

# Restart from scratch
.\setup.ps1  # Windows
# OR
./setup.sh   # Linux/macOS
```

---

## Contributing

### Development Workflow

1. **Create Feature Branch**
   ```bash
   git checkout -b feature/your-feature-name
   ```

2. **Make Changes**
   - Update code in `scripts/` or `airflow/dags/`
   - Test changes locally
   - Update documentation

3. **Run Tests**
   ```bash
   python scripts/test_system.py
   python scripts/test_connection.py
   ```

4. **Commit & Push**
   ```bash
   git add .
   git commit -m "feat: descriptive commit message"
   git push origin feature/your-feature-name
   ```

5. **Create Pull Request**
   - Provide clear description
   - Link related issues
   - Request code review

### Coding Standards

#### Python Style Guide

```python
# ‚úÖ Use type hints
def calculate_demand(orders: pd.DataFrame, stock: pd.DataFrame) -> pd.DataFrame:
    pass

# ‚úÖ Docstrings for functions
def export_orders(date: str) -> dict:
    """
    Export supplier orders for a specific date.
    
    Args:
        date (str): Processing date in YYYY-MM-DD format
        
    Returns:
        dict: Export results with file count and total units
    """
    pass

# ‚úÖ Business-meaningful variable names
net_demand = total_orders - available_stock + safety_stock

# ‚ùå Avoid generic names
result = x - y + z
```

#### SQL Style Guide

```sql
-- ‚úÖ Explicit catalog references
SELECT * FROM postgresql.procurement.products

-- ‚úÖ Uppercase keywords
SELECT sku, SUM(quantity) AS total
FROM hive.default.orders
WHERE order_date = DATE '2026-01-03'
GROUP BY sku

-- ‚úÖ Readable formatting
SELECT 
    o.sku,
    p.product_name,
    SUM(o.quantity) as demand
FROM hive.default.orders o
JOIN postgresql.procurement.products p 
    ON o.sku = p.sku
WHERE o.order_date = DATE '2026-01-03'
GROUP BY o.sku, p.product_name
```

### Testing Guidelines

```bash
# Always test before committing:

# 1. Syntax validation
python -m py_compile scripts/*.py

# 2. Connection tests
python scripts/test_connection.py

# 3. System validation
python scripts/test_system.py

# 4. Manual pipeline run
python scripts/run_pipeline.py --date 2026-01-03

# 5. Check outputs
ls -lh data/output/
cat data/output/pipeline_summary_2026-01-03.txt
```

---

## Additional Resources

### Documentation

- üìñ [HDFS_REPLICATION.md](procurement-pipeline/HDFS_REPLICATION.md) - HDFS 3-way replication details
- üìñ [PROJECT_STATUS.md](procurement-pipeline/PROJECT_STATUS.md) - Current project state
- üìñ [TODO.MD](TODO.MD) - Project roadmap and tasks
- üìñ [SUMMARY_REPORT.md](procurement-pipeline/SUMMARY_REPORT.md) - Pipeline execution reports

### Official Documentation

- **Apache Airflow**: https://airflow.apache.org/docs/
- **Trino**: https://trino.io/docs/current/
- **Apache Hadoop HDFS**: https://hadoop.apache.org/docs/stable/
- **PostgreSQL**: https://www.postgresql.org/docs/
- **Docker Compose**: https://docs.docker.com/compose/

### Useful Commands Cheat Sheet

```bash
# ===== DOCKER =====
docker-compose up -d              # Start all services
docker-compose down               # Stop all services
docker-compose ps                 # List containers
docker-compose logs -f service    # Follow logs

# ===== HDFS =====
hdfs dfs -ls /                    # List HDFS root
hdfs dfs -cat /path/file          # View file content
hdfs dfs -put local remote        # Upload file
hdfs dfsadmin -report             # Cluster report

# ===== TRINO =====
trino --execute "SHOW CATALOGS"   # List catalogs
trino --execute "SHOW TABLES FROM hive.default"

# ===== AIRFLOW =====
airflow dags list                 # List DAGs
airflow dags trigger dag_id       # Trigger DAG
airflow tasks test dag_id task_id date

# ===== POSTGRESQL =====
psql -U admin -d procurement_db   # Connect to DB
\dt                               # List tables
\d+ table_name                    # Describe table
```

---

## Contact

### Project Information

**Academic Institution**: √âcole Nationale des Sciences Appliqu√©es (ENSA) Al Hoceima  
**Department**: Computer Engineering & Data Science  
**Module**: Fondements Big Data (Big Data Fundamentals)  
**Academic Year**: 2025-2026  
**Project Type**: Batch Data Pipeline with Distributed Storage

### Support

For questions, issues, or collaboration:

- üìß **Email**: [Your University Email]
- üè´ **Institution**: ENSA Al Hoceima, Morocco
- üìö **Course**: Big Data Fundamentals
- üåê **Project Repository**: [GitHub Link]

### Acknowledgments

This project demonstrates:
- ‚úÖ **Distributed Storage**: HDFS with 3-way replication
- ‚úÖ **Federated Queries**: Trino querying multiple data sources
- ‚úÖ **Workflow Orchestration**: Apache Airflow DAG scheduling
- ‚úÖ **Data Engineering**: ETL pipeline with business logic
- ‚úÖ **Containerization**: Docker-based microservices architecture
- ‚úÖ **Best Practices**: Code quality, testing, documentation

**Special Thanks To**:
- Apache Software Foundation for Airflow, Hadoop, and related tools
- Trino Community for the powerful query engine
- Docker Inc. for containerization platform
- PostgreSQL Global Development Group
- Open Source Community

---

## üìú License

**Academic Project** - ENSA Al Hoceima ¬© 2025-2026

This project is developed for educational purposes as part of the Big Data Fundamentals course at √âcole Nationale des Sciences Appliqu√©es (ENSA) Al Hoceima.

### Usage Rights

- ‚úÖ Educational use permitted
- ‚úÖ Code may be studied and learned from
- ‚úÖ Attribution to ENSA Al Hoceima required
- ‚ùå Commercial use not permitted without authorization
- ‚ùå Redistribution requires permission

---

## üåü Project Highlights

### Technical Achievements

- ‚úÖ **Production-Grade**: Professional Docker-based infrastructure
- ‚úÖ **Highly Available**: 3-way HDFS replication with 4 datanodes
- ‚úÖ **Automated**: One-command setup with `setup.ps1`/`setup.sh`
- ‚úÖ **Monitored**: Comprehensive Airflow UI and logging
- ‚úÖ **Tested**: Validation scripts and health checks
- ‚úÖ **Documented**: 2000+ lines of comprehensive documentation
- ‚úÖ **Scalable**: Handles 141+ files across distributed storage
- ‚úÖ **Intelligent**: Business rules for procurement optimization

### Academic Learning Outcomes

Through this project, students learn:

1. **Distributed Systems**: HDFS architecture and replication
2. **Query Federation**: Trino's ability to query multiple sources
3. **Workflow Orchestration**: Airflow DAG design and scheduling
4. **Data Engineering**: ETL pipeline construction
5. **Containerization**: Docker Compose for microservices
6. **Big Data Concepts**: Batch processing, data lakes, OLTP vs OLAP
7. **SQL Optimization**: Query performance tuning
8. **Production Operations**: Monitoring, logging, troubleshooting

---

<div align="center">

### Built with Knowledge, Powered by Open Source

**ENSA Al Hoceima | Big Data Fundamentals | 2025-2026**

---

**[‚¨Ü Back to Top](#-big-data-procurement-pipeline)**

</div>
